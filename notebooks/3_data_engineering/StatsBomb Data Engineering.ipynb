{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering of StatsBomb Data\n",
    "##### Notebook to engineer previous parsed JSON data from the [StatsBomb Open Data GitHub repository](https://github.com/statsbomb/open-data)\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 10/11/2020<br>\n",
    "Notebook last updated: 18/02/2021\n",
    "\n",
    "![title](../../img/logos/stats-bomb-logo.png)\n",
    "\n",
    "Click [here](#section5) to jump straight to the Exploratory Data Analysis section and skip the [Task Brief](#section2), [Data Sources](#section3), and [Data Engineering](#section4) sections. Or click [here](#section6) to jump straight to the Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## <a id='import_libraries'>Introduction</a>\n",
    "This notebook parses pubicly available [StatsBomb](https://statsbomb.com/) Event data, using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "For more information about this notebook and the author, I'm available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/);\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster);\n",
    "*    [kaggle.com/eddwebster](https://www.kaggle.com/eddwebster); and\n",
    "*    [hackerrank.com/eddwebster](https://www.hackerrank.com/eddwebster).\n",
    "\n",
    "![title](../../img/edd_webster/fifa21eddwebsterbanner.png)\n",
    "\n",
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/football_analytics) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/football_analytics/blob/master/notebooks/2_data_parsing/StatsBomb%20Parsing%20and%20Data%20Engineering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Project Brief](#section2)<br>\n",
    "3.    [Data Sources](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Download the Data](#section3.2)<br>\n",
    "      3.    [Read in the Datasets](#section3.3)<br>\n",
    "      4.    [Join the Datasets](#section3.4)<br>\n",
    "      5.    [Initial Data Handling](#section3.5)<br>\n",
    "4.    [Data Engineering](#section4)<br>\n",
    "      1.    [Assign Raw DataFrame to Engineered DataFrame](#section4.1)<br>\n",
    "      2.    [Sort the DataFrame](#section4.2)<br>\n",
    "      3.    [Create Sort the DataFrame](#section4.3)<br>\n",
    "      4.    [Subset DataFrame](#section4.4)<br>\n",
    "5.    [Export DataFrame](#section5)<br>\n",
    "6.    [Summary](#section6)<br>\n",
    "7.    [Next Steps](#section7)<br>\n",
    "8.    [Bibliography](#section8)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section1'>1. Notebook Dependencies</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing;\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation; and\n",
    "*    `tqdm` for a clean progress bar;\n",
    "\n",
    "All packages used for this notebook except for BeautifulSoup can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python â‰¥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd    # version 1.0.3\n",
    "import os    #  used to read the csv filenames\n",
    "import re\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "import codecs\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Football Libraries\n",
    "from FCPython import createPitch\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno as msno    # visually display missing data\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm    # a clean progress bar library\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, Video, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6\n",
      "NumPy: 1.18.0\n",
      "pandas: 1.2.0\n",
      "matplotlib: 3.3.2\n",
      "Seaborn: 0.11.1\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "print('Seaborn: {}'.format(sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..', )\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_sb = os.path.join(base_dir, 'data', 'sb')\n",
    "scripts_dir = os.path.join(base_dir, 'scripts')\n",
    "scripts_dir_sb = os.path.join(base_dir, 'scripts', 'sb')\n",
    "data_dir_understat = os.path.join(base_dir, 'data', 'understat')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom function to read JSON files that also handles the encoding of special characters e.g. accents in names of players and teams\n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'rb') as json_file:\n",
    "        return BytesIO(json_file.read()).getvalue().decode('unicode_escape')\n",
    "    \n",
    "# Define custom function to flatten pandas DataFrames with nested JSON columns. Source: https://stackoverflow.com/questions/39899005/how-to-flatten-a-pandas-dataframe-with-some-columns-as-json\n",
    "def flatten_nested_json_df(df):\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    print(f\"original shape: {df.shape}\")\n",
    "    print(f\"original columns: {df.columns}\")\n",
    "\n",
    "\n",
    "    # search for columns to explode/flatten\n",
    "    s = (df.applymap(type) == list).all()\n",
    "    list_columns = s[s].index.tolist()\n",
    "\n",
    "    s = (df.applymap(type) == dict).all()\n",
    "    dict_columns = s[s].index.tolist()\n",
    "\n",
    "    print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "    while len(list_columns) > 0 or len(dict_columns) > 0:\n",
    "        new_columns = []\n",
    "\n",
    "        for col in dict_columns:\n",
    "            print(f\"flattening: {col}\")\n",
    "            # explode dictionaries horizontally, adding new columns\n",
    "            horiz_exploded = pd.json_normalize(df[col]).add_prefix(f'{col}.')\n",
    "            horiz_exploded.index = df.index\n",
    "            df = pd.concat([df, horiz_exploded], axis=1).drop(columns=[col])\n",
    "            new_columns.extend(horiz_exploded.columns) # inplace\n",
    "\n",
    "        for col in list_columns:\n",
    "            print(f\"exploding: {col}\")\n",
    "            # explode lists vertically, adding new columns\n",
    "            df = df.drop(columns=[col]).join(df[col].explode().to_frame())\n",
    "            new_columns.append(col)\n",
    "\n",
    "        # check if there are still dict o list fields to flatten\n",
    "        s = (df[new_columns].applymap(type) == list).all()\n",
    "        list_columns = s[s].index.tolist()\n",
    "\n",
    "        s = (df[new_columns].applymap(type) == dict).all()\n",
    "        dict_columns = s[s].index.tolist()\n",
    "\n",
    "        print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "\n",
    "    print(f\"final shape: {df.shape}\")\n",
    "    print(f\"final columns: {df.columns}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section2'>2. Project Brief</a>\n",
    "This Jupyter notebook explores how to parse publicly available Event data from [StatsBomb](https://statsbomb.com/) using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "\n",
    "The combined event data roduced in this notebook is exported to CSV. This data can be further analysed in Python, joined to other datasets, or explored using Tableau, PowerBI, Microsoft Excel.\n",
    "\n",
    "\n",
    "**Notebook Conventions**:<br>\n",
    "*    Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "*    Variables that refer to a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section3'>3. Data Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.1'>3.1. Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.1.1'>3.1.1. About StatsBomb</a>\n",
    "[StatsBomb](https://statsbomb.com/) are a football analytics and data company.\n",
    "\n",
    "![title](../../img/logos/stats-bomb-logo.png)\n",
    "\n",
    "Before conducting our EDA, the data needs to be imported as a DataFrame in the Data Sources section [Section 3](#section3) and Cleaned in the Data Engineering section [Section 4](#section4).\n",
    "\n",
    "We'll be using the [pandas](http://pandas.pydata.org/) library to import our data to this workbook as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.1.2'>3.1.2. About the StatsBomb publicly available data</a>\n",
    "The complete data set contains:\n",
    "- 7 competitions;\n",
    "- 879 matches;\n",
    "- 3,161,917 events; and\n",
    "- z players.\n",
    "\n",
    "The datasets we will be using are:\n",
    "- competitions;\n",
    "- matches;\n",
    "- events;\n",
    "- lineups; and\n",
    "- tactics;\n",
    "\n",
    "The data needs to be imported as a DataFrame in the Data Sources section [Section 3](#section3) and cleaned in the Data Engineering section [Section 4](#section4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.2'>3.2. Read in Data</a>\n",
    "The following cells read the the `JSON` files into a `DataFrame` object with some basic Data Engineering to flatten the data and select only the columns of interest, to ensure the notebook doesn't crash on a standard laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.3.1.'>3.3.1. Competitions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/sb/combined/raw/csv']\n"
     ]
    }
   ],
   "source": [
    "# Show files in directory\n",
    "print(glob.glob(os.path.join(data_dir_sb, 'combined', 'raw', 'csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (18,40,42,50,51,53,54,77,78,79,80,81,84,85,86,87,89,91,93,96,97,98,99,100,101,102,103,104,106,107,108,109,110,111,112,113,114,116,118,120,121,122,123,125,127,128,129,130,131,132,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,159,160,165,169,170,176,177,190) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file as a pandas DataFrame\n",
    "df_sb = pd.read_csv(os.path.join(data_dir_sb, 'combined', 'raw', 'csv', 'combined.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.3'>3.3. Initial Data Handling</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the raw DataFrame, df_sb\n",
    "df_sb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the raw DataFrame, df_sb\n",
    "df_sb.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the raw DataFrame, df_sb\n",
    "print(df_sb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the raw DataFrame, df_sb\n",
    "print(df_sb.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joined dataset has forty features (columns). Full details of these attributes can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of the features of the raw DataFrame, df_sb\n",
    "df_sb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full details of these attributes and their data types can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_sb\n",
    "df_sb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_sb, showing some summary statistics for each numberical column in the DataFrame\n",
    "df_sb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_sb\n",
    "msno.matrix(df_sb, figsize = (30, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_sb.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation shows us that there are no missing values in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section4'>4. Data Engineering</a>\n",
    "Before conducting an [Exploratory Data Analysis (EDA)](#section5) of the data, we'll first need to clean and wrangle the datasets to a form that meet our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.1'>4.1. Sort DataFrame</a>\n",
    "Sort data by `matchId`, `matchPeriod`, and `eventSec`. Important for when determining previous events. which are attributes created for the DataFrame in the Data Engineering notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by matchId, matchPeriod, and eventSec\n",
    "df_sb = df_sb.sort_values(['matchId', 'matchPeriod', 'eventSec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.3'>4.3. Create Engineered Attributes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.2.1'>4.2.1. Create `Team` and `Opponent` Attributes</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb['Team'] = np.where(df_sb['team.name'] == df_sb['home_team.home_team_name'], df_sb['home_team.home_team_name'], df_sb['away_team.away_team_name'])\n",
    "df_sb['Opponent'] = np.where(df_sb['team.name'] == df_sb['away_team.away_team_name'], df_sb['home_team.home_team_name'], df_sb['away_team.away_team_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.2.2'>4.2.2. Create `Full_Fixture_Date` Attribute</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb['Full_Fixture_Date'] = df_sb['match_date'].astype(str) + ' ' + df_sb['home_team.home_team_name'].astype(str)  + ' ' + df_sb['home_score'].astype(str) + ' ' + ' vs. ' + ' ' + df_sb['away_score'].astype(str) + ' ' + df_sb['away_team.away_team_name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.4'>4.4. Subset DataFrame</a>\n",
    "Subset DataFrame into\n",
    "- In-play Events only\n",
    "- Lineups - Starting XI\n",
    "- Tactical Changes\n",
    "- Halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb['type.name'] column\n",
    "df_sb['type.name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.4.1'>4.4.1. Isolate In-Play Events</a>\n",
    "DataFrame of only player's actions i.e. removing line ups, halves, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='#section4.4.1.1'>4.4.1.1. Remove Non-Event rows</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_events = ['Pass', 'Ball Receipt*', 'Carry', 'Duel', 'Miscontrol', 'Pressure', 'Ball Recovery', 'Dribbled Past', 'Dribble', 'Shot', 'Block', 'Goal Keeper', 'Clearance', 'Dispossessed', 'Foul Committed', 'Foul Won', 'Interception', 'Shield', 'Half End', 'Substitution', 'Tactical Shift', 'Injury Stoppage', 'Player Off', 'Player On', 'Offside', 'Referee Ball-Drop', 'Error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events = df_sb[df_sb['type.name'].isin(lst_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='#section4.4.1.2'>4.4.1.2. Break down all `location` attributes into seperate attribute for X, Y (and sometimes Z) coordinates</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all location columns\n",
    "for col in df_sb_events.columns:\n",
    "    if 'location' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the following five 'location' attributes:\n",
    "- `location`\n",
    "- `pass.end_location`\n",
    "- `carry.end_location`\n",
    "- `shot.end_location`\n",
    "- `goalkeeper.end_location`\n",
    "\n",
    "From reviewing the official documentation [[link](https://statsbomb.com/stat-definitions/)], the five attributes have the following dimensionality:\n",
    "- `location` [x, y]\n",
    "- `pass.end_location` [x, y]\n",
    "- `carry.end_location` [x, y]\n",
    "- `shot.end_location` [x, y, z]\n",
    "- `goalkeeper.end_location` [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# CURRENTLY NOT WORKING, NEED TO FIX\n",
    "\n",
    "# Normalize 'shot.freeze_frame' avvtribute - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_sb_events_normalize = df_sb_events.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['shot.freeze_frame']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    d = pd.json_normalize(df_sb_events_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_sb_events_normalize = pd.concat([df_sb_events_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_sb_events_normalize.head(30)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_events['location'] = df_sb_events['location'].astype(str)\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].astype(str)\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].astype(str)\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].astype(str)\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].astype(str)\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].astype(str)\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].astype(str)\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "###\n",
    "df_sb_events['location'] = df_sb_events['location'].str.replace('[','')\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].str.replace('[','')\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].str.replace('[','')\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].str.replace('[','')\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].str.replace('[','')\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].str.replace('[','')\n",
    "\n",
    "###\n",
    "df_sb_events['location'] = df_sb_events['location'].str.replace(']','')\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].str.replace(']','')\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].str.replace(']','')\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].str.replace(']','')\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].str.replace(']','')\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].str.replace(']','')\n",
    "\n",
    "\n",
    "## Break down each location attributes\n",
    "df_sb_events['location_x'], df_sb_events['location_y'] = df_sb_events['location'].str.split(',', 1).str\n",
    "df_sb_events['pass.end_location_x'], df_sb_events['pass.end_location_y'] = df_sb_events['pass.end_location'].str.split(',', 1).str\n",
    "df_sb_events['carry.end_location_x'], df_sb_events['carry.end_location_y'] = df_sb_events['carry.end_location'].str.split(',', 1).str\n",
    "df_sb_events['shot.end_location_x'], df_sb_events['shot.end_location_y'], df_sb_events['shot.end_location_z'] = df_sb_events['shot.end_location'].str.split(',', 3).str[0:3].str\n",
    "df_sb_events['goalkeeper.end_location_x'], df_sb_events['goalkeeper.end_location_y'] = df_sb_events['goalkeeper.end_location'].str.split(',', 1).str\n",
    "#df_sb_events['shot.freeze_frame_x'], df_sb_events['shot.freeze_frame_y'] = df_sb_events['shot.freeze_frame'].str.split(',', 1).str\n",
    "\n",
    "\n",
    "## Display DataFrame\n",
    "df_sb_events.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_sb_events.to_csv(data_dir_sb + '/events/engineered/' + '/sb_events_1819_2021_wsl.csv', index=None, header=True)\n",
    "\n",
    "# Export \n",
    "#df_sb_events.to_csv(data_dir + '/export/' + '/sb_wsl_events.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='#section4.4.1.3'>4.4.1.3. Create Passing Matrix Data</a>\n",
    "The following DataFrame is the CSV extract used for Tableau dashboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_sb_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['df_name'] = 'df1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_sb_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['df_name'] = 'df2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatanate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing['Pass_X'] = np.where(df_sb_events_passing['df_name'] == 'df1', df_sb_events_passing['location_x'], df_sb_events_passing['pass.end_location_x'])\n",
    "df_sb_events_passing['Pass_Y'] = np.where(df_sb_events_passing['df_name'] == 'df1', df_sb_events_passing['location_y'], df_sb_events_passing['pass.end_location_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_sb_events_passing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'export', 'sb_wsl_events_passing_matrix.csv')):\n",
    "    df_sb_events_passing.to_csv(os.path.join(data_dir_sb, 'export', 'sb_wsl_events_passing_matrix.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='#section4.4.1.4'>4.4.1.4. Create Passing Network Data</a>\n",
    "\n",
    "See: https://community.tableau.com/s/question/0D54T00000C6YbE/football-passing-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network = df_sb_events_passing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network = df_sb_pass_network[df_sb_pass_network['type.name'] == 'Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network['player_recipient'] = np.where(df_sb_pass_network['df_name'] == 'df1', df_sb_pass_network['player.name'], df_sb_pass_network['pass.recipient.name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_sb_pass_network.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['df_name',\n",
    "        'id',\n",
    "        'index',\n",
    "        'competition_name',\n",
    "        'season_name',\n",
    "        'match_date',\n",
    "        'kick_off',\n",
    "        'Full_Fixture_Date',\n",
    "        'Team',\n",
    "        'Opponent',\n",
    "        'home_team.home_team_name',\n",
    "        'away_team.away_team_name',\n",
    "        'home_score',\n",
    "        'away_score',\n",
    "        'player_recipient',\n",
    "        'player.name',\n",
    "        'pass.recipient.name',\n",
    "        'position.id',\n",
    "        'position.name',\n",
    "        'type.name',\n",
    "        'pass.type.name',\n",
    "        'pass.outcome.name',\n",
    "        'location_x',\n",
    "        'location_y', \n",
    "        'pass.end_location_x',\n",
    "        'pass.end_location_y',\n",
    "        'Pass_X',\n",
    "        'Pass_Y'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_select = df_sb_pass_network[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select['pass.to.from'] = df_sb_pass_network_select['player.name'] + ' - ' + df_sb_pass_network_select['pass.recipient.name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb_pass_network_select['pass.outcome.name'] column\n",
    "df_sb_pass_network_select['pass.outcome.name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select = df_sb_pass_network_select[df_sb_pass_network_select['pass.outcome.name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select = df_sb_pass_network_select.sort_values(['season_name', 'match_date', 'kick_off', 'Full_Fixture_Date', 'index', 'id', 'df_name'], ascending=[True, True, True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select['Pass_X'] = df_sb_pass_network_select['Pass_X'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['Pass_Y'] = df_sb_pass_network_select['Pass_Y'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['location_x'] = df_sb_pass_network_select['location_x'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['location_y'] = df_sb_pass_network_select['location_y'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['pass.end_location_x'] = df_sb_pass_network_select['pass.end_location_x'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['pass.end_location_y'] = df_sb_pass_network_select['pass.end_location_y'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = (df_sb_pass_network_select\n",
    "                                  .groupby(['competition_name',\n",
    "                                            'season_name',\n",
    "                                            'match_date',\n",
    "                                            'kick_off',\n",
    "                                            'Full_Fixture_Date',\n",
    "                                            'Team',\n",
    "                                            'Opponent',\n",
    "                                            'home_team.home_team_name',\n",
    "                                            'away_team.away_team_name',\n",
    "                                            'home_score',\n",
    "                                            'away_score',\n",
    "                                            'pass.to.from',\n",
    "                                            'player.name',\n",
    "                                            'pass.recipient.name',\n",
    "                                            'player_recipient'\n",
    "                                           ])\n",
    "                                  .agg({'pass.to.from': ['count']\n",
    "                                       })\n",
    "                             )\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped.columns = df_sb_pass_network_grouped.columns.droplevel(level=0)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = df_sb_pass_network_grouped.reset_index()\n",
    "\n",
    "## \n",
    "df_sb_pass_network_grouped.columns = ['competition_name',\n",
    "                                      'season_name',\n",
    "                                      'match_date',\n",
    "                                      'kick_off',\n",
    "                                      'full_fixture_date',\n",
    "                                      'team',\n",
    "                                      'opponent',\n",
    "                                      'home_team_name',\n",
    "                                      'away_team_name',\n",
    "                                      'home_score',\n",
    "                                      'away_score',\n",
    "                                      'pass_to_from',\n",
    "                                      'player_name',\n",
    "                                      'pass_recipient_name',\n",
    "                                      'player_recipient',\n",
    "                                      'count_passes',\n",
    "                                     ]\n",
    "\n",
    "##\n",
    "#df_sb_pass_network_grouped['count_passes'] = df_sb_pass_network_grouped['count_passes'] / 2\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = df_sb_pass_network_grouped.sort_values(['season_name', 'match_date', 'kick_off', 'full_fixture_date', 'team', 'opponent', 'pass_to_from'], ascending=[True, True, True, True, True, True, True])\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['Full_Fixture_Date',\n",
    "        'player.name',\n",
    "        'position.id',\n",
    "        'position.name',\n",
    "        'Pass_X',\n",
    "        'Pass_Y'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass = df_sb_pass_network_select[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_avg_pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = (df_sb_pass_network_avg_pass \n",
    "                                          .groupby(['Full_Fixture_Date',\n",
    "                                                    'player.name',\n",
    "                                                    'position.id',\n",
    "                                                    'position.name',\n",
    "                                                   ])\n",
    "                                          .agg({'Pass_X': ['mean'],\n",
    "                                                'Pass_Y': ['mean']\n",
    "                                               })\n",
    "                                     )\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped.columns = df_sb_pass_network_avg_pass_grouped .columns.droplevel(level=0)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = df_sb_pass_network_avg_pass_grouped.reset_index()\n",
    "\n",
    "## \n",
    "df_sb_pass_network_avg_pass_grouped.columns = ['full_fixture_date',\n",
    "                                               'player_name',\n",
    "                                               'position_id',\n",
    "                                               'position_name',\n",
    "                                               'avg_location_pass_x',\n",
    "                                               'avg_location_pass_y'\n",
    "                                     ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped['avg_location_pass_x'] = df_sb_pass_network_avg_pass_grouped['avg_location_pass_x'].round(decimals=1)\n",
    "df_sb_pass_network_avg_pass_grouped['avg_location_pass_y'] = df_sb_pass_network_avg_pass_grouped['avg_location_pass_y'].round(decimals=1)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = df_sb_pass_network_avg_pass_grouped.sort_values(['full_fixture_date', 'player_name'], ascending=[True, True])\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Events DataFrame to the Matches DataFrame\n",
    "df_sb_pass_network_final = pd.merge(df_sb_pass_network_grouped, df_sb_pass_network_avg_pass_grouped, left_on=['full_fixture_date', 'player_recipient'], right_on=['full_fixture_date', 'player_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "df_sb_pass_network_final = df_sb_pass_network_final.rename(columns={'player_name_x': 'player_name',\n",
    "                                                                   #'player_name_x': 'player_name'\n",
    "                                                                   }\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sb_pass_network_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'export', 'engineered', 'sb_events_passing_network.csv')):\n",
    "    df_sb_pass_network_final.to_csv(os.path.join(data_dir_sb, 'export', 'engineered', 'sb_events_passing_network.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'export', 'engineered', 'sb_events_passing_network.csv')):\n",
    "    df_sb_pass_network_final.to_csv(os.path.join(data_dir, 'export', 'sb_events_passing_network.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export WSL data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_sb_pass_network_final.to_csv(data_dir_sb + '/events/engineered/' + '/sb_events_passing_network_1819_2021_wsl.csv', index=None, header=True)\n",
    "\n",
    "# Export \n",
    "#df_sb_pass_network_final.to_csv(data_dir + '/export/' + '/sb_wsl_events_passing_network.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.4.2'>4.4.2. Lineups</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb['type.name'] column\n",
    "df_sb['type.name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting XI players and formation can be found in the rows where `type.name` is 'Starting XI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup = df_sb[df_sb['type.name'] == 'Starting XI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline DataFrame to include just the columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['id', 'type.name', 'match_date', 'kick_off', 'Full_Fixture_Date', 'team.id', 'team.name', 'tactics.formation', 'tactics.lineup', 'competition_name', 'season_name', 'home_team.home_team_name', 'away_team.away_team_name', 'Team', 'Opponent', 'home_score', 'away_score']\n",
    "\n",
    "## Select only columns of interest\n",
    "df_lineup_select = df_lineup[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the extracted lineup data so far. To get the stating XI players, we need to breakdown the `tactics.lineup` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize tactics.lineup - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_lineup_select_normalize = df_lineup_select.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['tactics.lineup']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    d = pd.json_normalize(df_lineup_select_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_lineup_select_normalize = pd.concat([df_lineup_select_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_lineup_select_normalize.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered = df_lineup_select_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline DataFrame to include just the columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['id', 'match_date', 'kick_off', 'Full_Fixture_Date', 'type.name', 'season_name', 'competition_name', 'home_team.home_team_name', 'away_team.away_team_name', 'Team', 'Opponent', 'home_score', 'away_score', 'tactics.formation', 'tactics.lineup_jersey_number', 'tactics.lineup_position_id', 'tactics.lineup_player_name', 'tactics.lineup_position_name']\n",
    "\n",
    "## Select only columns of interest\n",
    "df_lineup_engineered_select = df_lineup_engineered[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lineup_engineered_select['tactics.formation'] = df_lineup_engineered_select['tactics.formation'].astype('Int64')\n",
    "df_lineup_engineered_select['tactics.lineup_jersey_number'] = df_lineup_engineered_select['tactics.lineup_jersey_number'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.rename(columns={'id': 'Match_Id',\n",
    "                                                                          'match_date': 'Match_Date',\n",
    "                                                                          'kick_off': 'Kick_Off',\n",
    "                                                                          'type.name': 'Type_Name',\n",
    "                                                                          'season_name': 'Season',\n",
    "                                                                          'competition_name': 'Competition',\n",
    "                                                                          'home_team.home_team_name': 'Home_Team',\n",
    "                                                                          'away_team.away_team_name': 'Away_Team',\n",
    "                                                                          'home_score': 'Home_Score',\n",
    "                                                                          'away_score': 'Away_Score',\n",
    "                                                                          'tactics.formation': 'Formation',\n",
    "                                                                          'tactics.lineup_jersey_number': 'Shirt_Number',\n",
    "                                                                          'tactics.lineup_position_id': 'Position_Number',\n",
    "                                                                          'tactics.lineup_player_name': 'Player_Name',\n",
    "                                                                          'tactics.lineup_position_name': 'Position_Name'\n",
    "                                                                         }\n",
    "                                                                         \n",
    "                                                                )\n",
    "\n",
    "## Display DataFrame\n",
    "df_lineup_engineered_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Match_Date from string to datetime64[ns]\n",
    "df_lineup_engineered_select['Match_Date']= pd.to_datetime(df_lineup_engineered_select['Match_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# THIS IS NOT WORKING ATM\n",
    "\n",
    "# Convert Kick_Off from string to datetime64[ns]\n",
    "df_lineup_engineered_select['Kick_Off']= pd.to_datetime(df_lineup_engineered_select['Kick_Off'], format='%H:%M', errors='ignore')\n",
    "df_lineup_engineered_select['Kick_Off'] = df_lineup_engineered_select['Kick_Off'].dt.time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put hyphens between numbers in Formation attribute\n",
    "\n",
    "## Convert Formation attribute from Integer to String\n",
    "df_lineup_engineered_select['Formation'] = df_lineup_engineered_select['Formation'].astype(str)\n",
    "\n",
    "## Define custom function to add hyphen between letters: StackOverflow: https://stackoverflow.com/questions/29382285/python-making-a-function-that-would-add-between-letters\n",
    "def f(s):\n",
    "        m = s[0]\n",
    "        for i in s[1:]:\n",
    "             m += '-' + i\n",
    "        return m\n",
    "    \n",
    "## Apply custom function\n",
    "df_lineup_engineered_select['Formation'] = df_lineup_engineered_select.apply(lambda row: f(row['Formation']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_formation = df_lineup_engineered_select['Formation'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Position Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formations_coords = pd.read_csv(data_dir_sb + '/sb_formation_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_formations_coords['Id'] = df_formations_coords['Id'].astype('Int8')\n",
    "#df_formations_coords['Player_Number'] = df_formations_coords['Player_Number'].astype('Int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select = pd.merge(df_lineup_engineered_select, df_formations_coords, how='left', left_on=['Formation', 'Position_Number'], right_on=['Formation', 'Player_Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lineup_engineered_select = df_lineup_engineered_select.drop(['Player_Number'], axis=1)\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.drop(['Id'], axis=1)\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.drop(['Player_Position'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Opponent Data to Each Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['Match_Date',\n",
    "        'Competition',\n",
    "        'Full_Fixture_Date',\n",
    "        'Team',\n",
    "        'Formation'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_lineup_opponent = df_lineup_engineered_select[cols]\n",
    "\n",
    "##\n",
    "df_lineup_opponent = df_lineup_opponent.drop_duplicates()\n",
    "\n",
    "##\n",
    "df_lineup_opponent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join DataFrame to itself on 'Date', 'Fixture', 'Team'/'Opponent', and 'Event', to join Team and Opponent together\n",
    "df_lineup_engineered_opponent_select = pd.merge(df_lineup_engineered_select, df_lineup_opponent,  how='left', left_on=['Match_Date', 'Competition', 'Full_Fixture_Date', 'Opponent'], right_on = ['Match_Date', 'Competition', 'Full_Fixture_Date', 'Team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "\n",
    "## Drop columns\n",
    "df_lineup_engineered_opponent_select = df_lineup_engineered_opponent_select.drop(columns=['Team_y'])\n",
    "\n",
    "\n",
    "## Rename columns\n",
    "df_lineup_engineered_opponent_select = df_lineup_engineered_opponent_select.rename(columns={'Team_x': 'Team',\n",
    "                                                                                            'Formation_x': 'Formation',\n",
    "                                                                                            'Formation_y': 'Opponent_Formation'\n",
    "                                                                                           }\n",
    "                                                                                      )\n",
    "\n",
    "## Display DataFrame\n",
    "df_lineup_engineered_opponent_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_lineup_engineered_opponent_select.to_csv(data_dir_sb + '/lineups/engineered/' + '/sb_lineups_1819_2021_wsl.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_lineup_engineered_opponent_select.to_csv(data_dir + '/export/' + '/sb_wsl_lineups.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.4.3'>4.4.3. Tactical Shifts</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics = df_sb[df_sb['type.name'] == 'Tactical Shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "##\n",
    "cols = ['id', 'type.name', 'team.id', 'team.name', 'tactics.formation', 'tactics.lineup']\n",
    "\n",
    "##\n",
    "df_tactics_select = df_tactics[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize tactics.lineup - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_tactics_select_normalize = df_tactics_select.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['tactics.lineup']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "for col in cols_to_normalize:\n",
    "    \n",
    "    d = pd.json_normalize(df_tactics_select_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_tactics_select_normalize = pd.concat([df_tactics_select_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_tactics_select_normalize.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section4.4.4'>4.4.4. Halves</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half = df_sb[df_sb['type.name'] == 'Half Start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section5'>5. Export Data</a>\n",
    "Export Data ready for data engineering in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_sb.to_csv(data_dir_sb + '/combined/raw/csv/wsl/' + '/df_sb_combined_data_wsl.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section6'>6. Summary</a>\n",
    "This notebook engineers scraped [StatsBomb](https://statsbomb.com/) data using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='#section7'>7. Next Steps</a>\n",
    "The step is to take the parsed dataset created in this notebook and engineer the data for new features, which is carried out in the follow [Data Engineering](https://nbviewer.jupyter.org/github/eddwebster/football_analytics/blob/master/notebooks/3_data_engineering/StatsBomb%20Data%20Engineering.ipynb) notebook. This data is then ready for use in projects including Expected Goals (xG) models and Tableau visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section8'>8. References</a>\n",
    "\n",
    "#### Data\n",
    "*    [StatsBomb](https://statsbomb.com/) data\n",
    "*    [StatsBomb](https://github.com/statsbomb/open-data/tree/master/data) open data GitHub repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [EddWebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "40px",
    "left": "1118px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
