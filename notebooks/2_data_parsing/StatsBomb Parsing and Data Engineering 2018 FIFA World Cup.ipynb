{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing of StatsBomb World Cup Data\n",
    "##### Notebook to parse 2018 FIFA World Cup JSON data from the [StatsBomb Open Data GitHub repository](https://github.com/statsbomb/open-data) to create one unified Events data DataFrame.\n",
    "\n",
    "### By [Edd Webster](https://www.twitter.com/eddwebster)\n",
    "Notebook first written: 06/07/2020<br>\n",
    "Notebook last updated: 09/07/2021\n",
    "\n",
    "![StatsBomb](../../img/logos/stats-bomb-logo.png)\n",
    "\n",
    "![title](../../img/fifaworldcup2018.jpg)\n",
    "\n",
    "Click [here](#section5) to jump straight to the Exploratory Data Analysis section and skip the [Task Brief](#section2), [Data Sources](#section3), and [Data Engineering](#section4) sections. Or click [here](#section6) to jump straight to the Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## <a id='import_libraries'>Introduction</a>\n",
    "This notebook parses pubicly available [StatsBomb](https://statsbomb.com/) Event data, using [pandas](http://pandas.pydata.org/) for data manipulation through DataFrames.\n",
    "\n",
    "For more information about this notebook and the author, I'm available through all the following channels:\n",
    "*    [eddwebster.com](https://www.eddwebster.com/);\n",
    "*    edd.j.webster@gmail.com;\n",
    "*    [@eddwebster](https://www.twitter.com/eddwebster);\n",
    "*    [linkedin.com/in/eddwebster](https://www.linkedin.com/in/eddwebster/);\n",
    "*    [github/eddwebster](https://github.com/eddwebster/);\n",
    "*    [public.tableau.com/profile/edd.webster](https://public.tableau.com/profile/edd.webster);\n",
    "*    [kaggle.com/eddwebster](https://www.kaggle.com/eddwebster); and\n",
    "*    [hackerrank.com/eddwebster](https://www.hackerrank.com/eddwebster).\n",
    "\n",
    "![title](../../img/edd_webster/fifa21eddwebsterbanner.png)\n",
    "\n",
    "The accompanying GitHub repository for this notebook can be found [here](https://github.com/eddwebster/football_analytics) and a static version of this notebook can be found [here](https://nbviewer.jupyter.org/github/eddwebster/football_analytics/blob/master/notebooks/2_data_parsing/StatsBomb%20Parsing%20and%20Data%20Engineering%202018%20FIFA%20World%20Cup.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## <a id='notebook_contents'>Notebook Contents</a>\n",
    "1.    [Notebook Dependencies](#section1)<br>\n",
    "2.    [Project Brief](#section2)<br>\n",
    "3.    [Data Sources](#section3)<br>\n",
    "      1.    [Introduction](#section3.1)<br>\n",
    "      2.    [Read in the Datasets](#section3.2)<br>\n",
    "      3.    [Join the Datasets](#section3.3)<br>\n",
    "      4.    [Initial Data Handling](#section3.4)<br>\n",
    "4.    [Data Engineering](#section4)<br>\n",
    "      1.    [Assign Raw DataFrame to Engineered DataFrame](#section4.1)<br>\n",
    "      2.    [Sort the DataFrame](#section4.2)<br>\n",
    "      3.    [Create New Attributes](#section4.3)<br>\n",
    "      4.    [Fill Null Values](#section4.4)<br>\n",
    "      5.    [Determine Each Player's Most Frequent Position](#section4.5)<br>\n",
    "5.    [Aggregated Data](#section5)<br>\n",
    "      1.    [](#section5.1)<br>\n",
    "      2.    [](#section5.2)<br>\n",
    "6.    [Subset Data](#section6)<br>\n",
    "7.    [Summary](#section7)<br>\n",
    "8.    [Next Steps](#section8)<br>\n",
    "9.    [References](#section9)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a id='section1'></a>\n",
    "\n",
    "## <a id='#section1'>1. Notebook Dependencies</a>\n",
    "\n",
    "This notebook was written using [Python 3](https://docs.python.org/3.7/) and requires the following libraries:\n",
    "*    [`Jupyter notebooks`](https://jupyter.org/) for this notebook environment with which this project is presented;\n",
    "*    [`NumPy`](http://www.numpy.org/) for multidimensional array computing;\n",
    "*    [`pandas`](http://pandas.pydata.org/) for data analysis and manipulation; and\n",
    "*    `tqdm` for a clean progress bar;\n",
    "\n",
    "All packages used for this notebook except for BeautifulSoup can be obtained by downloading and installing the [Conda](https://anaconda.org/anaconda/conda) distribution, available on all platforms (Windows, Linux and Mac OSX). Step-by-step guides on how to install Anaconda can be found for Windows [here](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) and Mac [here](https://medium.com/@GalarnykMichael/install-python-on-mac-anaconda-ccd9f2014072), as well as in the Anaconda documentation itself [here](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python â‰¥3.5 (ideally)\n",
    "import platform\n",
    "import sys, getopt\n",
    "assert sys.version_info >= (3, 5)\n",
    "import csv\n",
    "\n",
    "# Import Dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "# Math Operations\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd    # version 1.0.3\n",
    "import os    #  used to read the csv filenames\n",
    "import re\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Reading directories\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Working with JSON\n",
    "import json\n",
    "import codecs\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Football Libraries\n",
    "from FCPython import createPitch\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno as msno    # visually display missing data\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm    # a clean progress bar library\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import Image, Video, YouTubeVideo\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6\n",
      "NumPy: 1.18.0\n",
      "pandas: 1.2.0\n",
      "matplotlib: 3.3.2\n",
      "Seaborn: 0.11.1\n"
     ]
    }
   ],
   "source": [
    "# Python / module versions used here for reference\n",
    "print('Python: {}'.format(platform.python_version()))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "print('Seaborn: {}'.format(sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define today's date\n",
    "today = datetime.datetime.now().strftime('%d/%m/%Y').replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial paths to subfolders\n",
    "base_dir = os.path.join('..', '..', )\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "data_dir_sb = os.path.join(base_dir, 'data', 'sb')\n",
    "scripts_dir = os.path.join(base_dir, 'scripts')\n",
    "scripts_dir_sb = os.path.join(base_dir, 'scripts', 'sb')\n",
    "data_dir_understat = os.path.join(base_dir, 'data', 'understat')\n",
    "img_dir = os.path.join(base_dir, 'img')\n",
    "fig_dir = os.path.join(base_dir, 'img', 'fig')\n",
    "video_dir = os.path.join(base_dir, 'video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure\n",
    "for folder in ['combined', 'competitions', 'events', 'related', 'freeze', 'tactics', 'lineups']:\n",
    "    path = os.path.join(data_dir_sb, 'raw', folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom function to read JSON files that also handles the encoding of special characters e.g. accents in names of players and teams\n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'rb') as json_file:\n",
    "        return BytesIO(json_file.read()).getvalue().decode('unicode_escape')\n",
    "    \n",
    "# Define custom function to flatten pandas DataFrames with nested JSON columns. Source: https://stackoverflow.com/questions/39899005/how-to-flatten-a-pandas-dataframe-with-some-columns-as-json\n",
    "def flatten_nested_json_df(df):\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    print(f\"original shape: {df.shape}\")\n",
    "    print(f\"original columns: {df.columns}\")\n",
    "\n",
    "\n",
    "    # search for columns to explode/flatten\n",
    "    s = (df.applymap(type) == list).all()\n",
    "    list_columns = s[s].index.tolist()\n",
    "\n",
    "    s = (df.applymap(type) == dict).all()\n",
    "    dict_columns = s[s].index.tolist()\n",
    "\n",
    "    print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "    while len(list_columns) > 0 or len(dict_columns) > 0:\n",
    "        new_columns = []\n",
    "\n",
    "        for col in dict_columns:\n",
    "            print(f\"flattening: {col}\")\n",
    "            # explode dictionaries horizontally, adding new columns\n",
    "            horiz_exploded = pd.json_normalize(df[col]).add_prefix(f'{col}.')\n",
    "            horiz_exploded.index = df.index\n",
    "            df = pd.concat([df, horiz_exploded], axis=1).drop(columns=[col])\n",
    "            new_columns.extend(horiz_exploded.columns) # inplace\n",
    "\n",
    "        for col in list_columns:\n",
    "            print(f\"exploding: {col}\")\n",
    "            # explode lists vertically, adding new columns\n",
    "            df = df.drop(columns=[col]).join(df[col].explode().to_frame())\n",
    "            new_columns.append(col)\n",
    "\n",
    "        # check if there are still dict o list fields to flatten\n",
    "        s = (df[new_columns].applymap(type) == list).all()\n",
    "        list_columns = s[s].index.tolist()\n",
    "\n",
    "        s = (df[new_columns].applymap(type) == dict).all()\n",
    "        dict_columns = s[s].index.tolist()\n",
    "\n",
    "        print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n",
    "\n",
    "    print(f\"final shape: {df.shape}\")\n",
    "    print(f\"final columns: {df.columns}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>\n",
    "\n",
    "## <a id='#section2'>2. Project Brief</a>\n",
    ">Norwich City are a possession based team and therefore ball playing centre backs play a vital role within our game model. For the first stage of the interview process you are required to identify the top three ball playing centre backs from the [2018 FIFA World Cup](https://www.fifa.com/worldcup/archive/russia2018). Your analysis should include the selection of relevant position specific metrics/KPI's, data analysis and visualisation to communicate your analysis.\n",
    ">\n",
    ">For this practical task you are required to use [StatsBomb](https://statsbomb.com/) 's open data from the FIFA World Cup, 2018 which can be accessed as follows;\n",
    ">\n",
    ">*    Sign up to gain access to their open data via the following URL: https://statsbomb.com/academy/\n",
    ">*    Access all the necessary resources via Statsbomb's GitHub site: https://github.com/statsbomb/open-data\n",
    ">\n",
    ">Please use any presentation methods you deem suitable to compete this task. Finally please detail your process and methodology used for this task as an appendix to the main presentation.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Notebook Conventions**:<br>\n",
    "*    Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "*    Variables that refer to a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>\n",
    "\n",
    "## <a id='#section3'>3. Data Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.1'>3.1. Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.1.1'>3.1.1. About StatsBomb</a>\n",
    "[StatsBomb](https://statsbomb.com/) are a football analytics and data company.\n",
    "\n",
    "![title](../../img/logos/stats-bomb-logo.png)\n",
    "\n",
    "Before conducting our EDA, the data needs to be imported as a DataFrame in the Data Sources section [Section 3](#section3) and Cleaned in the Data Engineering section [Section 4](#section4).\n",
    "\n",
    "We'll be using the [pandas](http://pandas.pydata.org/) library to import our data to this workbook as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.1.2'>3.1.2. About the StatsBomb publicly available data</a>\n",
    "The complete data set contains:\n",
    "- 7 competitions;\n",
    "- 879 matches;\n",
    "- 3,161,917 events; and\n",
    "- z players.\n",
    "\n",
    "The datasets we will be using are:\n",
    "- competitions;\n",
    "- matches;\n",
    "- events;\n",
    "- lineups; and\n",
    "- tactics;\n",
    "\n",
    "The data needs to be imported as a DataFrame in the Data Sources section [Section 3](#section3) and cleaned in the Data Engineering section [Section 4](#section4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.2'>3.2. Read in Data</a>\n",
    "The following cells read the the `JSON` files into a `DataFrame` object with some basic Data Engineering to flatten the data and select only the columns of interest, to ensure the notebook doesn't crash on a standard laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.3.1.'>3.3.1. Competitions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MARKDOWN TABLE OF DATA HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show files in directory\n",
    "print(glob.glob(os.path.join(data_dir_sb, 'raw', 'competitions/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in exported CSV file if exists, if not, read in JSON file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'competitions', 'competitions_wc2018.csv')):\n",
    "    json_competitions = read_json_file(os.path.join(data_dir_sb, 'open-data', 'data', 'competitions.json'))\n",
    "    df_competitions_flat = pd.read_json(json_competitions)\n",
    "else:\n",
    "    df_competitions_flat = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'competitions', 'competitions_wc2018.csv'))    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_competitions_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competitions_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify 2018 FIFA World Cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame for rows where 'competition_name' is equal to 'FIFA World Cup'\n",
    "df_competitions_flat_wc2018 = df_competitions_flat.loc[df_competitions_flat['competition_name'] == 'FIFA World Cup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competitions_flat_wc2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify Competitions of Interest by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FA Women's Super League has competition ID 37\n",
    "competition_id = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we want just the 2018 FIFA World Cup which has the competition_id - `43`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'competitions', 'competitions_wc2018.csv')):\n",
    "    df_competitions_flat.to_csv(os.path.join(data_dir_sb, 'raw', 'competitions', 'competitions_wc2018.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.3.2.'>3.3.2. Matches</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MARKDOWN TABLE OF DATA HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define competitions\n",
    "The following cell lists the competitions to be included in the dataset. Dataset includes data for seven different competitions - 5 domestic and 2 international."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to select only the competitions of interest. \n",
    "\n",
    "# Define list of competitions\n",
    "lst_competitions = [#2,     # Premier League\n",
    "                    #11,    # La Liga\n",
    "                    #16,    # Champions League\n",
    "                    #37,    # FA Women's Super League\n",
    "                    43,    # FIFA World Cup\n",
    "                    #49,    # NWSL\n",
    "                    #72,    # Women's World Cup\n",
    "                   ]\n",
    "\n",
    "# Flatmap all competition IDs to use all available competitions\n",
    "#lst_competitions = df_competitions['competition_id'].unique().tolist()\n",
    "\n",
    "# Display list of competitions\n",
    "lst_competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst_competitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show files in directory\n",
    "print(glob.glob(os.path.join(data_dir_sb, 'raw', 'matches/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary solution because the loop below doesn't work\n",
    "\n",
    "# Read in exported CSV file if exists, if not, read in JSON file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv')):\n",
    "    \n",
    "    ## Import all StatsBomb JSON Match data for the World Cup 2018\n",
    "    with open(data_dir_sb + '/open-data/data/matches/' + str(competition_id) + '/3.json') as f:\n",
    "        json_sb_match_data_wc_2018 = json.load(f)\n",
    "\n",
    "    ## Flatten the JSON Events data\n",
    "    df_matches_flat = json_normalize(json_sb_match_data_wc_2018)\n",
    "\n",
    "    # Flatten the nested columns\n",
    "    #df_matches_flat = flatten_nested_json_df(df_matches_flat)\n",
    "\n",
    "    # Rename columns\n",
    "    #df_matches_flat.columns = df_matches_flat.columns.str.replace('[.]', '_')    # commented out for now\n",
    "    \n",
    "else:    \n",
    "    df_matches_flat = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv'))\n",
    "    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_matches_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Read in exported CSV file if exists, if not, read in JSON file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv')):\n",
    "    # Loop through the competition files for the selected competition(s)\n",
    "    # Take the separate JSON files each representing a match for the selected competition(s).\n",
    "    # The file is called {match_id}.json.\n",
    "    # Read the corresponding JSON matches files using the auxillary function\n",
    "    # Read JSON file as a pandas DataFrame\n",
    "    # Append the DataFrames to a list\n",
    "    # Finally, concatenate all the separate DataFrames into one DataFrame\n",
    "\n",
    "    ## Create empty list for DataFrames\n",
    "    dfs_matches = []\n",
    "\n",
    "    ## Loop through the competition files for the selected competition(s) and append DataFrame to dfs_matches list\n",
    "    for competition_id in lst_competitions:\n",
    "        filepath_competition = data_dir_sb + 'open-data/data/matches/' + str(competition_id)\n",
    "        filepath_matches = (glob.glob(filepath_competition + '/*.json'))\n",
    "        for filepath_match in filepath_matches:\n",
    "            df_match = pd.read_json(filepath_match)\n",
    "            dfs_matches.append(df_match)\n",
    "\n",
    "    ## Concatenate DataFrames to one DataFrame\n",
    "    df_matches = pd.concat(dfs_matches)\n",
    "    \n",
    "    # Flatten the nested columns\n",
    "    df_matches_flat = flatten_nested_json_df(df_matches)\n",
    "    \n",
    "    ## Rename columns\n",
    "    #df_matches_flat.columns = df_matches_flat.columns.str.replace('[.]', '_')    # commented out for now\n",
    "    \n",
    "else:    \n",
    "    df_matches_flat = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv'))\n",
    "    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_matches_flat.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert `match_id` column to list\n",
    "List used as reference of matches to parse for Events, Lineups, and Tactics data - iteration through list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatmap all competition IDs to use all available competitions\n",
    "lst_matches = df_matches_flat['match_id'].tolist()\n",
    "\n",
    "# Display list of competitions\n",
    "lst_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv')):\n",
    "    df_matches_flat.to_csv(os.path.join(data_dir_sb, 'raw', 'matches', 'matches_wc2018.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.3.3.'>3.3.3. Events</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [StatsBomb](https://statsbomb.com/) dataset has one hundred and fourteen features (columns) with the following definitions and data types:\n",
    "\n",
    "| Feature     | Data type    |\n",
    "|------|-----|\n",
    "| `id`    | `object`\n",
    "| `index`    | `object`\n",
    "| `period`    | `object`\n",
    "| `timestamp`    | `object`\n",
    "| `minute`    | `object`\n",
    "| `second`    | `object`\n",
    "| `possession`    | `object`\n",
    "| `duration`    | `object`\n",
    "| `type.id`    | `object`\n",
    "| `type.name`    | `object`\n",
    "| `possession_team.id`    | `object`\n",
    "| `possession_team.name`    | `object`\n",
    "| `play_pattern.id`    | `object`\n",
    "| `play_pattern.name`    | `object`\n",
    "| `team.id`    | `object`\n",
    "| `team.name`    | `object`\n",
    "| `tactics.formation`    | `object`\n",
    "| `tactics.lineup`    | `object`\n",
    "| `related_events`    | `object`\n",
    "| `location`    | `object`\n",
    "| `player.id`    | `object`\n",
    "| `player.name`    | `object`\n",
    "| `position.id`    | `object`\n",
    "| `position.name`    | `object`\n",
    "| `pass.recipient.id`    | `object`\n",
    "| `pass.recipient.name`    | `object`\n",
    "| `pass.length`    | `object`\n",
    "| `pass.angle`    | `object`\n",
    "| `pass.height.id`    | `object`\n",
    "| `pass.height.name`    | `object`\n",
    "| `pass.end_location`    | `object`\n",
    "| `pass.type.id`    | `object`\n",
    "| `pass.type.name`    | `object`\n",
    "| `pass.body_part.id`    | `object`\n",
    "| `pass.body_part.name`    | `object`\n",
    "| `carry.end_location`    | `object`\n",
    "| `under_pressure`    | `object`\n",
    "| `duel.type.id`    | `object`\n",
    "| `duel.type.name`    | `object`\n",
    "| `out`    | `object`\n",
    "| `miscontrol.aerial_won`    | `object`\n",
    "| `pass.outcome.id`    | `object`\n",
    "| `pass.outcome.name`    | `object`\n",
    "| `ball_receipt.outcome.id`    | `object`\n",
    "| `ball_receipt.outcome.name`    | `object`\n",
    "| `pass.aerial_won`    | `object`\n",
    "| `counterpress`    | `object`\n",
    "| `off_camera`    | `object`\n",
    "| `dribble.outcome.id`    | `object`\n",
    "| `dribble.outcome.name`    | `object`\n",
    "| `dribble.overrun`    | `object`\n",
    "| `ball_recovery.offensive`    | `object`\n",
    "| `shot.statsbomb_xg`    | `object`\n",
    "| `shot.end_location`    | `object`\n",
    "| `shot.outcome.id`    | `object`\n",
    "| `shot.outcome.name`    | `object`\n",
    "| `shot.type.id`    | `object`\n",
    "| `shot.type.name`    | `object`\n",
    "| `shot.body_part.id`    | `object`\n",
    "| `shot.body_part.name`    | `object`\n",
    "| `shot.technique.id`    | `object`\n",
    "| `shot.technique.name`    | `object`\n",
    "| `shot.freeze_frame`    | `object`\n",
    "| `goalkeeper.end_location`    | `object`\n",
    "| `goalkeeper.type.id`    | `object`\n",
    "| `goalkeeper.type.name`    | `object`\n",
    "| `goalkeeper.position.id`    | `object`\n",
    "| `goalkeeper.position.name`    | `object`\n",
    "| `pass.straight`    | `object`\n",
    "| `pass.technique.id`    | `object`\n",
    "| `pass.technique.name`    | `object`\n",
    "| `clearance.head`    | `object`\n",
    "| `clearance.body_part.id`    | `object`\n",
    "| `clearance.body_part.name`    | `object`\n",
    "| `pass.switch`    | `object`\n",
    "| `duel.outcome.id`    | `object`\n",
    "| `duel.outcome.name`    | `object`\n",
    "| `foul_committed.advantage`    | `object`\n",
    "| `foul_won.advantage`    | `object`\n",
    "| `pass.cross`    | `object`\n",
    "| `pass.assisted_shot_id`    | `object`\n",
    "| `pass.shot_assist`    | `object`\n",
    "| `shot.one_on_one`    | `object`\n",
    "| `shot.key_pass_id`    | `object`\n",
    "| `goalkeeper.body_part.id`    | `object`\n",
    "| `goalkeeper.body_part.name`    | `object`\n",
    "| `goalkeeper.technique.id`    | `object`\n",
    "| `goalkeeper.technique.name`    | `object`\n",
    "| `goalkeeper.outcome.id`    | `object`\n",
    "| `goalkeeper.outcome.name`    | `object`\n",
    "| `clearance.aerial_won`    | `object`\n",
    "| `foul_committed.card.id`    | `object`\n",
    "| `foul_committed.card.name`    | `object`\n",
    "| `foul_won.defensive`    | `object`\n",
    "| `clearance.right_foot`    | `object`\n",
    "| `shot.first_time`    | `object`\n",
    "| `pass.through_ball`    | `object`\n",
    "| `interception.outcome.id`    | `object`\n",
    "| `interception.outcome.name`    | `object`\n",
    "| `clearance.left_foot`    | `object`\n",
    "| `ball_recovery.recovery_failure`    | `object`\n",
    "| `shot.aerial_won`    | `object`\n",
    "| `pass.goal_assist`    | `object`\n",
    "| `pass.cut_back`    | `object`\n",
    "| `pass.deflected`    | `object`\n",
    "| `clearance.other`    | `object`\n",
    "| `pass.outswinging`    | `object`\n",
    "| `substitution.outcome.id`    | `object`\n",
    "| `substitution.outcome.name`    | `object`\n",
    "| `substitution.replacement.id`    | `object`\n",
    "| `substitution.replacement.name`    | `object`\n",
    "| `block.deflection`    | `object`\n",
    "| `block.offensive`    | `object`\n",
    "| `injury_stoppage.in_chain`    | `object`\n",
    "\n",
    "For a full list of definitions, see the official documentation [[link](https://statsbomb.com/stat-definitions/)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show files in directory\n",
    "print(glob.glob(os.path.join(data_dir_sb, 'raw', 'events/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in exported CSV file if exists, if not, read in JSON file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'events', 'events_wc2018.csv')):\n",
    "    # Loop through the matches files for the selected match(es)\n",
    "    # Take the separate JSON file each representing theevents match for the selected matches.\n",
    "    # The file is called {match_id}.json.\n",
    "    # Read the corresponding JSON matches files using the auxillary function\n",
    "    # Read JSON file as a pandas DataFrame\n",
    "    # Append the DataFrames to a list\n",
    "    # Finally, concatenate all the separate DataFrames into one DataFrame\n",
    "\n",
    "    ## Create empty list for DataFrames\n",
    "    dfs_events = []\n",
    "\n",
    "    ## Loop through event files for the selected matches and append DataFrame to dfs_events list\n",
    "    for match_id in lst_matches:\n",
    "        with open(data_dir_sb + '/open-data/data/events/' + str(match_id) + '.json') as f:\n",
    "            event = json.load(f)\n",
    "           #match_id = str(match_id)\n",
    "            df_event_flat = json_normalize(event)\n",
    "            df_event_flat['match_id'] = match_id\n",
    "            dfs_events.append(df_event_flat)    \n",
    "\n",
    "    ## Concatenate DataFrames to one DataFrame\n",
    "    df_events = pd.concat(dfs_events)\n",
    "    \n",
    "    # Flatten the nested columns\n",
    "    df_events_flat = flatten_nested_json_df(df_events)\n",
    "    \n",
    "    ## Rename columns\n",
    "    #df_events_flat.columns = df_events_flat.columns.str.replace('[.]', '_')    \n",
    "    \n",
    "else:    \n",
    "    df_events_flat = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'events', 'events_wc2018.csv'))\n",
    "    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_events_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'events', 'events_2018.csv')):\n",
    "    df_events_flat.to_csv(os.path.join(data_dir_sb, 'raw', 'events', 'events_2018.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View all formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatmap all formations\n",
    "lst_formation = df_events_flat['tactics.formation'].tolist()\n",
    "\n",
    "# Display list of competitions\n",
    "lst_formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section3.3.3.'>3.3.4. Lineups</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MARKDOWN TABLE OF DATA HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show files in directory\n",
    "print(glob.glob(os.path.join(data_dir_sb, 'raw', 'lineups/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in exported CSV file if exists, if not, read in JSON file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'lineups', 'lineups_wc2018.csv')):\n",
    "    # Loop through the competition files for the selected competition(s)\n",
    "    # Take the separate JSON files each representing a match for the selected competition(s).\n",
    "    # The file is called {match_id}.json.\n",
    "    # Read the corresponding JSON matches files using the auxillary function\n",
    "    # Read JSON file as a pandas DataFrame\n",
    "    # Append the DataFrames to a list\n",
    "    # Finally, concatenate all the separate DataFrames into one DataFrame\n",
    "\n",
    "    ## Create empty list for DataFrames\n",
    "    dfs_lineups = []\n",
    "\n",
    "    ## Loop through event files for the selected matches and append DataFrame to dfs_lineups list\n",
    "    for match_id in lst_matches:\n",
    "        with open(data_dir_sb + '/open-data/data/lineups/' + str(match_id) + '.json') as f:\n",
    "            lineup = json.load(f)\n",
    "           #match_id = str(match_id)\n",
    "            df_lineups_flat = json_normalize(lineup)\n",
    "            df_lineups_flat['match_id'] = match_id\n",
    "            dfs_lineups.append(df_lineups_flat)    \n",
    "\n",
    "    ## Concatenate DataFrames to one DataFrame\n",
    "    df_lineups = pd.concat(dfs_lineups)\n",
    "\n",
    "    # Flatten the nested columns\n",
    "    df_lineups_flat = flatten_nested_json_df(df_lineups)\n",
    "    \n",
    "    ## Rename columns\n",
    "    #df_lineups_flat.columns = df_lineups_flat.columns.str.replace('[.]', '_')    # commented out for now\n",
    "    \n",
    "else:    \n",
    "    df_lineups_flat = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'lineups', 'lineups_wc2018.csv'))\n",
    "    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_lineups_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineups_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'lineups', 'lineups_wc2018.csv')):\n",
    "    df_lineups_flat.to_csv(os.path.join(data_dir_sb, 'raw', 'lineups', 'lineups_wc2018.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.4'>3.4. Join Datasets</a>\n",
    "Next, we're required to join the `Matches` DataFrame and the `Players` DataFrame to the `Events` DatFrame. The `Events` data is the base DataFrame in which we join the other tables via `x`, `y`, `z`, `z`, and `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in exported CSV file if exists, if not, merge the individual DataFrames\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'combined', 'combined_wc2018.csv')):\n",
    "    \n",
    "    # Join the Matches DataFrame to the Events DataFrame\n",
    "    df_events_matches = pd.merge(df_events_flat, df_matches_flat, left_on=['match_id'], right_on=['match_id'])\n",
    "\n",
    "    # Join the Competitions DataFrame to the Events-Matches DataFrame\n",
    "    df_events_matches_competitions = pd.merge(df_events_matches, df_competitions_flat, left_on=['competition.competition_id', 'season.season_id'], right_on=['competition_id', 'season_id'])\n",
    "    \n",
    "else:    \n",
    "    df_events_matches_competitions = pd.read_csv(os.path.join(data_dir_sb, 'raw', 'combined', 'combined_wc2018.csv'))\n",
    "    \n",
    "    \n",
    "# Display DataFrame\n",
    "df_events_matches_competitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_matches_competitions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "if not os.path.exists(os.path.join(data_dir_sb, 'raw', 'combined', 'combined_wc2018.csv')):\n",
    "    df_events_matches_competitions.to_csv(os.path.join(data_dir_sb, 'raw', 'combined', 'combined_wc2018.csv'), index=None, header=True)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section3.5'>3.5. Initial Data Handling</a>\n",
    "Let's quality of the dataset by looking first and last rows in pandas using the [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the DataFrame, df_events_matches_competitions\n",
    "df_events_matches_competitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five rows of the DataFrame, df_events_matches_competitions\n",
    "df_events_matches_competitions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame, df_events_matches_competitions\n",
    "print(df_events_matches_competitions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the DataFrame, df_events_matches_competitions\n",
    "print(df_events_matches_competitions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joined dataset has forty features (columns). Full details of these attributes can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of the features of the raw DataFrame, df_events_matches_competitions\n",
    "df_events_matches_competitions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full details of these attributes and their data types can be found in the [Data Dictionary](section3.3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info for the raw DataFrame, df_events_matches_competitions\n",
    "#df_events_matches_competitions.info()    # commented out as it crashes the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the raw DataFrame, df_events_matches_competitions, showing some summary statistics for each numberical column in the DataFrame\n",
    "#df_events_matches_competitions.describe()    # commented out as it crashes the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualisation of the missing values for each feature of the raw DataFrame, df_events_matches_competitions\n",
    "#msno.matrix(df_events_matches_competitions, figsize = (30, 7))    # commented out as it crashes the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of missing values\n",
    "null_value_stats = df_events_matches_competitions.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section4'></a>\n",
    "\n",
    "## <a id='#section4'>4. Data Engineering</a>\n",
    "Before conducting an [Exploratory Data Analysis (EDA)](#section5) of the data, we'll first need to clean and wrangle the datasets to a form that meet our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.1'>4.1. Assign Raw DataFrame to Engineered DataFrame</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Raw DataFrame to Engineered DataFrame\n",
    "df_sb = df_events_matches_competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.2'>4.2. Sort DataFrame</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = df_sb.sort_values(['Full_Fixture_Date', 'match_date', 'timestamp'], ascending=[True, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.3'>4.3. Create New Attributes</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb['Team'] = np.where(df_sb['team.name'] == df_sb['home_team.home_team_name'], df_sb['home_team.home_team_name'], df_sb['away_team.away_team_name'])\n",
    "df_sb['Opponent'] = np.where(df_sb['team.name'] == df_sb['away_team.away_team_name'], df_sb['home_team.home_team_name'], df_sb['away_team.away_team_name'])\n",
    "\n",
    "df_sb['Full_Fixture_Date'] = df_sb['match_date'].astype(str) + ' ' + df_sb['home_team.home_team_name'].astype(str)  + ' ' + df_sb['home_score'].astype(str) + ' ' + ' vs. ' + ' ' + df_sb['away_score'].astype(str) + ' ' + df_sb['away_team.away_team_name'].astype(str)\n",
    "\n",
    "df_sb['next_event'] = df_sb['type.name'].shift(-1)\n",
    "df_sb['previous_event'] = df_sb['type.name'].shift(+1)\n",
    "df_sb['next_team_possession'] = df_sb['possession_team.name'].shift(-1)\n",
    "df_sb['previous_team_possession'] = df_sb['possession_team.name'].shift(+1)\n",
    "df_sb['possession_retained'] = np.where((df_sb['possession_team.name'] == df_sb['next_team_possession']), 1, 0)\n",
    "df_sb['xA'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['next_event'] == 'Shot')), df_sb['shot.statsbomb_xg'], 0)\n",
    "df_sb['key_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['next_event'] == 'Shot')), 1, 0)\n",
    "\n",
    "df_sb['complete_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.outcome.name'] == 'Complete')), 1, 0)\n",
    "df_sb['incomplete_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.outcome.name'] == 'Incomplete')), 1, 0)\n",
    "df_sb['attempted_pass'] = df_sb['complete_pass'] + df_sb['incomplete_pass']\n",
    "df_sb['complete_pressured_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['under_pressure'] == True) & (df_sb['pass.outcome.name'] == 'Complete')), 1, 0)\n",
    "df_sb['incomplete_pressured_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['under_pressure'] == True) & (df_sb['pass.outcome.name'] == 'Incomplete')), 1, 0)\n",
    "df_sb['attempted_pressured_pass'] = df_sb['complete_pressured_pass'] + df_sb['incomplete_pressured_pass']\n",
    "df_sb['complete_pass_distance'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['complete_pass'] == 1)), df_sb['pass.length'], 0)\n",
    "df_sb['progressive_pass'] = np.where((df_sb['type.name'] == 'Pass') & (df_sb['pass.angle'] > 0), 1, 0)\n",
    "df_sb['progressive_pass_distance'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['complete_pass'] == 1) & (df_sb['progressive_pass'] == 1)), df_sb['pass.length'], 0)\n",
    "df_sb['completed_short_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['complete_pass'] == 1) & (df_sb['pass.length'] >= 5) & (df_sb['pass.length'] <= 15)), 1, 0)\n",
    "df_sb['attempted_short_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.length'] >= 5) & (df_sb['pass.length'] <= 15)), 1, 0)\n",
    "df_sb['completed_medium_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['complete_pass'] == 1) & (df_sb['pass.length'] >= 15) & (df_sb['pass.length'] <= 30)), 1, 0)\n",
    "df_sb['attempted_medium_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.length'] > 15) & (df_sb['pass.length'] <= 30)), 1, 0)\n",
    "df_sb['completed_long_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['complete_pass'] == 1) & (df_sb['pass.length'] > 30)), 1, 0)\n",
    "df_sb['attempted_long_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.length'] > 30)), 1, 0)\n",
    "df_sb['assisted_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.goal_assist'] == True)), 1, 0)\n",
    "df_sb['xA'] = np.where(df_sb['next_event'] == 'Shot', df_sb['shot.statsbomb_xg'], 0)    # fix expected assists\n",
    "df_sb['tackle'] = np.where(((df_sb['type.name'] == 'Duel') & (df_sb['duel.type.name'] == 'Tackle')), 1, 0)\n",
    "df_sb['interception'] = np.where(df_sb['type.name'] == 'Interception', 1, 0)\n",
    "df_sb['dribbled_past'] = np.where(df_sb['type.name'] == 'Dribbled Past', 1, 0)\n",
    "df_sb['open_play_pass'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['play_pattern.name'] == 'Regular Play')), 1, 0) \n",
    "df_sb['carry_attempted'] = np.where(df_sb['type.name'] == 'Carry', 1, 0)\n",
    "df_sb['carry_completed'] = np.where(((df_sb['type.name'] == 'Carry') & (df_sb['possession_retained'] == 1)), 1, 0)\n",
    "df_sb['carry_length'] = np.where(df_sb['type.name'] == 'Carry', np.sqrt((df_sb['location_y'] - df_sb['carry.end_location_y']) ** 2 + (df_sb['location_x'] - df_sb['carry.end_location_x']) ** 2), 0)\n",
    "df_sb['clearance'] = np.where(df_sb['type.name'] == 'Clearance', 1, 0)\n",
    "#df_sb['aerial_duel_won'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.body_part.name'] == 'Head')), 1, 0)    # check this one, not so sure\n",
    "df_sb['aerial_duel_won'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['pass.aerial_won'] == True)), 1, 0)    # check this one, not so sure\n",
    "df_sb['aerial_duel_lost'] = np.where(((df_sb['type.name'] == 'Duel') & (df_sb['duel.type.name'] == 'Aerial Lost')), 1, 0)    # check this one, not so sure\n",
    "df_sb['pressured_long_ball'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['under_pressure'] == True) & (df_sb['pass.height.name'] == 'High Pass')), 1, 0)\n",
    "df_sb['unpressured_long_ball'] = np.where(((df_sb['type.name'] == 'Pass') & (df_sb['under_pressure'] == False) & (df_sb['pass.height.name'] == 'High Pass')), 1, 0)\n",
    "df_sb['pressure'] = np.where(df_sb['type.name'] == 'Pressure', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.4'>4.4. Fill Null Values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb['pass.outcome.name'] = df_sb['pass.outcome.name'].fillna('Complete')\n",
    "df_sb['under_pressure'] = df_sb['under_pressure'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section4.5'>4.5. Determine Each Player's Most Frequent Position</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Each Player's Most Frequeny Position\n",
    "\n",
    "## Groupby and Aggregate by player name and position\n",
    "df_sb_player_positions = (df_sb\n",
    "                              .groupby(['player.name', 'position.name'])\n",
    "                              .agg({'type.name': 'count'})\n",
    "                              .reset_index()\n",
    "                         )\n",
    "\n",
    "## Rename columns after groupby and aggregation\n",
    "df_sb_player_positions.columns = ['player', 'position', 'count']\n",
    "\n",
    "## Take the high occuring position per player\n",
    "df_sb_player_positions = (df_sb_player_positions\n",
    "                              .groupby(['player', 'position'])\n",
    "                              .apply(lambda x: x.sort_values(['count'], ascending=False))\n",
    "                              .reset_index(drop=True)\n",
    "                              .groupby(['player']).head(1)\n",
    "                              .drop(['count'], axis=1)\n",
    "                         )\n",
    "## Display DataFrame\n",
    "df_sb_player_positions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section5'></a>\n",
    "\n",
    "## <a id='#section5'>5. Aggregate Data</a>\n",
    "\n",
    "Twelve key metrics for player assessment and their definitions are as follows:\n",
    "*    **Tackles & Dribbles Past p90**: ...\n",
    "*    **Dribblers tackled %**: the number of dribblers tackled divided by dribblers tackled plus times dribbled past;\n",
    "*    **Aerial Wins %**: the percentage of aerial battles won divided by total aerial battles;\n",
    "*    **Aerial Wins p90**: the number of aerial duels a player wins, per 90 minutes;\n",
    "*    **Open Play Passes p90**: the number of attempted passes in open play, per 90 minutes;\n",
    "*    **Pass Completion %**: the number of completed passes divded by the number of attempted passes;\n",
    "*    **Being Pressured Change in Pass %**: How does passing % change when under pressure? This is calculated as Pressured Pass % minus Pass %\n",
    "*    **Deep Progressions p90**: the number of passes and dribbles/carries into the opposition final third, p90\n",
    "*    **xGBuildup p90**: xG Chain is the total xG of every possession the player is involved in. xG build up is the same minus shots and key passes. To determine this: 1.Find all the possessions each player is involved in, 2.Find all the shots within those possessions, 3.Sum their xG (you might take the highest xG per possession, or you might treat the shots as dependent events), and 4.Assign that sum to each player, however involved they were.\n",
    "*    **Carries p90**: the number of carries, defined as when a player controls the ball at their feet while moving or standing still, p90;\n",
    "*    **Carry %**: percentage of a player's Carries that were successful; and\n",
    "*    **Carry Length p90**: average Carry length, p90.\n",
    "\n",
    "Other metrics considered for defenders:\n",
    "*    **Fouls p90**: the number of fouls per 90 minutes\n",
    "*    **Pressures p90**: the number of times applying pressure to opposing player who is receiving, carrying or releasing the ball\n",
    "*    **Pressured Long Balls**: \n",
    "*    **Unpressured Long Balls**: the number of completed long balls while not under pressure per 90.\n",
    "*    **Tackles p90**: the number of tackles per 90 minutes (ideally this would be pAdj Tackles, the number of tackles adjusted proportionally to the possession volume of a team. Unfortunately, in the time available for this task, this is difficult to determine);\n",
    "*    **Interceptions**: the number of interceptions per 90 minutes (ideally this would be pAdj Interceptions, the number of interceptions adjusted proportionally to the possession volume of a team. Unfortunately, in the time available for this task, this is difficult to determine); \n",
    "*    **Average Defensive Action Distance**: the average distance from the goal line that the player successfully makes a defensive action;\n",
    "*    **Clearances p90**: the number of times a player makes a clearance or plays a long ball while under pressure, per 90 minutes;\n",
    "*    **Blocks p90**: the number of blocks, per 90 minutes. A 'block' is defined as blocking the ball by standing in its path; and\n",
    "*    **Blocks/Shot p90**: the number of blocks made per shot faced, per 90 minutes. \n",
    "\n",
    "Baseline attributes required:\n",
    "*    **Player**: the player's name;\n",
    "*    **Team**: the team or in this case, the country that the player is playing for;\n",
    "*    **Opponent**: the team or in this case, the country that the player is playing against;\n",
    "*    **Full Fixture Date**: ;\n",
    "*    **Minutes played**: the number of minutes played; and\n",
    "*    **Games played**: the total number of matches played (for the aggregated version only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.1'>5.1. Groupby and Aggregate by Player and Match</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Passing DataFrame\n",
    "\n",
    "## Groupby and aggregate\n",
    "df_sb_match_by_match = (df_sb\n",
    "                           .groupby(['player.name',\n",
    "                                     'Team',\n",
    "                                     'Opponent',\n",
    "                                     'Full_Fixture_Date',\n",
    "                                    ]\n",
    "                                   )\n",
    "                           .agg({'minute': ['min', 'max'],\n",
    "                                 'complete_pass': ['sum'],\n",
    "                                 'incomplete_pass': ['sum'],\n",
    "                                 'attempted_pass': ['sum'],\n",
    "                                 'complete_pressured_pass': ['sum'],\n",
    "                                 'incomplete_pressured_pass': ['sum'],\n",
    "                                 'attempted_pressured_pass': ['sum'],\n",
    "                                 'complete_pass_distance': ['sum'],\n",
    "                                 'progressive_pass_distance': ['sum', 'mean'],\n",
    "                                 'completed_short_pass': ['sum'],\n",
    "                                 'attempted_short_pass': ['sum'],\n",
    "                                 'completed_medium_pass': ['sum'],\n",
    "                                 'attempted_medium_pass': ['sum'],\n",
    "                                 'completed_long_pass': ['sum'],\n",
    "                                 'attempted_long_pass': ['sum'],\n",
    "                                 'assisted_pass': ['sum'],\n",
    "                                 'xA': ['sum'],\n",
    "                                 'key_pass': ['sum'],\n",
    "                                 'shot.statsbomb_xg': ['sum'],\n",
    "                                 'tackle': ['sum'],\n",
    "                                 'interception': ['sum'],\n",
    "                                 'dribbled_past': ['sum'],\n",
    "                                 'open_play_pass': ['sum'],\n",
    "                                 'carry_attempted': ['sum'],\n",
    "                                 'carry_completed': ['sum'],\n",
    "                                 'carry_length': ['sum'],\n",
    "                                 'clearance': ['sum'],\n",
    "                                 'aerial_duel_won': ['sum'],\n",
    "                                 'aerial_duel_lost': ['sum'],\n",
    "                                 'pressured_long_ball': ['sum'],\n",
    "                                 'unpressured_long_ball': ['sum'],\n",
    "                                 'pressure': ['sum'],\n",
    "                                 'Full_Fixture_Date': ['nunique']\n",
    "                                }\n",
    "                               )\n",
    "                       )\n",
    "\n",
    "## Drop level\n",
    "df_sb_match_by_match.columns = df_sb_match_by_match.columns.droplevel(level=0)\n",
    "\n",
    "## Reset index\n",
    "df_sb_match_by_match = df_sb_match_by_match.reset_index()\n",
    "\n",
    "## Rename columns\n",
    "df_sb_match_by_match.columns = ['player',\n",
    "                                'team',\n",
    "                                'opponent',\n",
    "                                'full_fixture_date',\n",
    "                                'min_start',\n",
    "                                'min_end',\n",
    "                                'passes_completed',\n",
    "                                'passes_incompleted',\n",
    "                                'passes_attempted',\n",
    "                                'pressured_passes_completed',\n",
    "                                'pressured_passes_incompleted',\n",
    "                                'pressured_passes_attempted',\n",
    "                                'total_distance_completed_passes',\n",
    "                                'total_distance_progressive_passes',\n",
    "                                'average_distance_progressive_passes',\n",
    "                                'short_passes_completed',\n",
    "                                'short_passes_attempted',\n",
    "                                'medium_passes_completed',\n",
    "                                'medium_passes_attempted',\n",
    "                                'long_passes_completed',\n",
    "                                'long_passes_attempted',\n",
    "                                'assists',\n",
    "                                'expected_assists',\n",
    "                                'key_passes',\n",
    "                                'expected_goals',\n",
    "                                'tackles',\n",
    "                                'interceptions',\n",
    "                                'dribbled_past',\n",
    "                                'open_play_passes',\n",
    "                                'carries',\n",
    "                                'carries_completed',\n",
    "                                'carry_length',\n",
    "                                'clearances',\n",
    "                                'aerial_duels_won',\n",
    "                                'aerial_duels_lost',\n",
    "                                'pressured_long_balls',\n",
    "                                'unpressured_long_balls',\n",
    "                                'pressures',\n",
    "                                'games_played'\n",
    "                               ]\n",
    "\n",
    "## Replace values\n",
    "df_sb_match_by_match['min_start'] = np.where(df_sb_match_by_match['min_start'] <= 5, 0, df_sb_match_by_match['min_start'])    # fix this\n",
    "\n",
    "## Create new attributes post-aggregation\n",
    "df_sb_match_by_match['minutes'] = df_sb_match_by_match['min_end'] - df_sb_match_by_match['min_start']\n",
    "df_sb_match_by_match['pass_completion_percentage'] = ((df_sb_match_by_match['passes_completed'] / (df_sb_match_by_match['passes_completed'] + df_sb_match_by_match['passes_incompleted'])) * 100).round(1)\n",
    "df_sb_match_by_match['pressured_pass_completion_percentage'] = ((df_sb_match_by_match['pressured_passes_completed'] / (df_sb_match_by_match['pressured_passes_completed'] + df_sb_match_by_match['pressured_passes_incompleted'])) * 100).round(1)\n",
    "df_sb_match_by_match['pressure_change_in_pass_completion_percentage'] = df_sb_match_by_match['pressured_pass_completion_percentage'] - df_sb_match_by_match['pass_completion_percentage'].round(1) \n",
    "df_sb_match_by_match['short_pass_completion_percentage'] = ((df_sb_match_by_match['short_passes_completed'] / df_sb_match_by_match['short_passes_attempted']) * 100).round(1)\n",
    "df_sb_match_by_match['medium_pass_completion_percentage'] = ((df_sb_match_by_match['medium_passes_completed'] / df_sb_match_by_match['medium_passes_attempted']) * 100).round(1)\n",
    "df_sb_match_by_match['long_pass_completion_percentage'] = ((df_sb_match_by_match['long_passes_completed'] / df_sb_match_by_match['long_passes_attempted']) * 100).round(1)\n",
    "df_sb_match_by_match['assist_minus_expected_assist'] = df_sb_match_by_match['assists'] - df_sb_match_by_match['expected_assists']\n",
    "df_sb_match_by_match['tackles_and_interceptions'] = df_sb_match_by_match['tackles'] + df_sb_match_by_match['interceptions']\n",
    "df_sb_match_by_match['tackles_and_dribbles_past'] = (df_sb_match_by_match['tackles'] / (df_sb_match_by_match['tackles'] + df_sb_match_by_match['dribbled_past']) * 100).round(1)\n",
    "df_sb_match_by_match['carry_completion_percentage'] = ((df_sb_match_by_match['carries_completed'] / df_sb_match_by_match['carries']) * 100).round(1)\n",
    "df_sb_match_by_match['aerial_duels_win_percentage'] = ((df_sb_match_by_match['aerial_duels_won'] / (df_sb_match_by_match['aerial_duels_won'] + df_sb_match_by_match['aerial_duels_lost'])) * 100).round(1)\n",
    "\n",
    "## Create 'per 90' metrics\n",
    "df_sb_match_by_match['tackles_and_interceptions_p90'] = (df_sb_match_by_match['tackles_and_interceptions'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['open_play_passes_p90'] = (df_sb_match_by_match['open_play_passes'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['carries_p90'] = (df_sb_match_by_match['carries'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['carry_length_p90'] = (df_sb_match_by_match['carry_length'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['aerial_duels_won_p90'] = (df_sb_match_by_match['aerial_duels_won'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['pressured_long_balls_p90'] = (df_sb_match_by_match['pressured_long_balls'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['unpressured_long_balls_p90'] = (df_sb_match_by_match['unpressured_long_balls'] / df_sb_match_by_match['minutes']) * 90\n",
    "df_sb_match_by_match['pressures_p90'] = (df_sb_match_by_match['pressures'] / df_sb_match_by_match['minutes']) * 90\n",
    "\n",
    "## Remove columns\n",
    "df_sb_match_by_match = df_sb_match_by_match.drop(['min_start', 'min_end'], axis=1)\n",
    "\n",
    "## Round figures\n",
    "df_sb_match_by_match['total_distance_progressive_passes'] = df_sb_match_by_match['total_distance_progressive_passes'].round(1)\n",
    "df_sb_match_by_match['average_distance_progressive_passes'] = df_sb_match_by_match['average_distance_progressive_passes'].round(1)\n",
    "df_sb_match_by_match['total_distance_completed_passes'] = df_sb_match_by_match['total_distance_completed_passes'].round(0)\n",
    "df_sb_match_by_match['expected_goals'] = df_sb_match_by_match['expected_goals'].round(3)\n",
    "df_sb_match_by_match['tackles_and_interceptions_p90'] = df_sb_match_by_match['tackles_and_interceptions_p90'].round(2)\n",
    "df_sb_match_by_match['open_play_passes_p90'] = df_sb_match_by_match['open_play_passes_p90'].round(1)\n",
    "df_sb_match_by_match['carries_p90'] = df_sb_match_by_match['carries_p90'].round(1)\n",
    "df_sb_match_by_match['carry_length_p90'] = df_sb_match_by_match['carry_length_p90'].round(1)\n",
    "df_sb_match_by_match['aerial_duels_won_p90'] = df_sb_match_by_match['aerial_duels_won_p90'].round(1)\n",
    "df_sb_match_by_match['pressured_long_balls_p90'] = df_sb_match_by_match['pressured_long_balls_p90'].round(1)\n",
    "df_sb_match_by_match['unpressured_long_balls_p90'] = df_sb_match_by_match['unpressured_long_balls_p90'].round(1)\n",
    "df_sb_match_by_match['pressures_p90'] = df_sb_match_by_match['pressures_p90'].round(1)\n",
    "\n",
    "## Join player positions to DataFrame\n",
    "df_sb_match_by_match = pd.merge(df_sb_match_by_match, df_sb_player_positions, how='left', left_on=['player'], right_on=['player'])\n",
    "\n",
    "## Display DataFrame\n",
    "df_sb_match_by_match.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.2'>5.2. Groupby and Aggregate by Player for the Entire Tournament</a>\n",
    "*    Replace `df_sb_match_by_match` for `df_sb_all`.\n",
    "*    Comment out #'Opponent' and #'Full_Fixture_Date',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Passing DataFrame\n",
    "\n",
    "## Groupby and aggregate\n",
    "df_sb_all = (df_sb\n",
    "                   .groupby(['player.name',\n",
    "                             'Team',\n",
    "                            ]\n",
    "                           )\n",
    "                   .agg({'minute': ['min', 'max'],\n",
    "                         'complete_pass': ['sum'],\n",
    "                         'incomplete_pass': ['sum'],\n",
    "                         'attempted_pass': ['sum'],\n",
    "                         'complete_pressured_pass': ['sum'],\n",
    "                         'incomplete_pressured_pass': ['sum'],\n",
    "                         'attempted_pressured_pass': ['sum'],\n",
    "                         'complete_pass_distance': ['sum'],\n",
    "                         'progressive_pass_distance': ['sum', 'mean'],\n",
    "                         'completed_short_pass': ['sum'],\n",
    "                         'attempted_short_pass': ['sum'],\n",
    "                         'completed_medium_pass': ['sum'],\n",
    "                         'attempted_medium_pass': ['sum'],\n",
    "                         'completed_long_pass': ['sum'],\n",
    "                         'attempted_long_pass': ['sum'],\n",
    "                         'assisted_pass': ['sum'],\n",
    "                         'xA': ['sum'],\n",
    "                         'key_pass': ['sum'],\n",
    "                         'shot.statsbomb_xg': ['sum'],\n",
    "                         'tackle': ['sum'],\n",
    "                         'interception': ['sum'],\n",
    "                         'dribbled_past': ['sum'],\n",
    "                         'open_play_pass': ['sum'],\n",
    "                         'carry_attempted': ['sum'],\n",
    "                         'carry_completed': ['sum'],\n",
    "                         'carry_length': ['sum'],\n",
    "                         'clearance': ['sum'],\n",
    "                         'aerial_duel_won': ['sum'],\n",
    "                         'aerial_duel_lost': ['sum'],\n",
    "                         'pressured_long_ball': ['sum'],\n",
    "                         'unpressured_long_ball': ['sum'],\n",
    "                         'pressure': ['sum'],\n",
    "                         'Full_Fixture_Date': ['nunique']\n",
    "                        }\n",
    "                       )\n",
    "               )\n",
    "\n",
    "## Drop level\n",
    "df_sb_all.columns = df_sb_all.columns.droplevel(level=0)\n",
    "\n",
    "## Reset index\n",
    "df_sb_all = df_sb_all.reset_index()\n",
    "\n",
    "## Rename columns\n",
    "df_sb_all.columns = ['player',\n",
    "                    'team',\n",
    "                    'min_start',\n",
    "                    'min_end',\n",
    "                    'passes_completed',\n",
    "                    'passes_incompleted',\n",
    "                    'passes_attempted',\n",
    "                    'pressured_passes_completed',\n",
    "                    'pressured_passes_incompleted',\n",
    "                    'pressured_passes_attempted',\n",
    "                    'total_distance_completed_passes',\n",
    "                    'total_distance_progressive_passes',\n",
    "                    'average_distance_progressive_passes',\n",
    "                    'short_passes_completed',\n",
    "                    'short_passes_attempted',\n",
    "                    'medium_passes_completed',\n",
    "                    'medium_passes_attempted',\n",
    "                    'long_passes_completed',\n",
    "                    'long_passes_attempted',\n",
    "                    'assists',\n",
    "                    'expected_assists',\n",
    "                    'key_passes',\n",
    "                    'expected_goals',\n",
    "                    'tackles',\n",
    "                    'interceptions',\n",
    "                    'dribbled_past',\n",
    "                    'open_play_passes',\n",
    "                    'carries',\n",
    "                    'carries_completed',\n",
    "                    'carry_length',\n",
    "                    'clearances',\n",
    "                    'aerial_duels_won',\n",
    "                    'aerial_duels_lost',\n",
    "                    'pressured_long_balls',\n",
    "                    'unpressured_long_balls',\n",
    "                    'pressures',\n",
    "                    'games_played'\n",
    "                   ]\n",
    "\n",
    "## Replace values\n",
    "df_sb_all['min_start'] = np.where(df_sb_all['min_start'] <= 5, 0, df_sb_all['min_start'])    # fix this\n",
    "\n",
    "## Create new attributes post-aggregation\n",
    "df_sb_all['minutes'] = df_sb_all['min_end'] - df_sb_all['min_start']\n",
    "df_sb_all['pass_completion_percentage'] = ((df_sb_all['passes_completed'] / (df_sb_all['passes_completed'] + df_sb_all['passes_incompleted'])) * 100).round(1)\n",
    "df_sb_all['pressured_pass_completion_percentage'] = ((df_sb_all['pressured_passes_completed'] / (df_sb_all['pressured_passes_completed'] + df_sb_all['pressured_passes_incompleted'])) * 100).round(1)\n",
    "df_sb_all['pressure_change_in_pass_completion_percentage'] = df_sb_all['pressured_pass_completion_percentage'] - df_sb_all['pass_completion_percentage'].round(1) \n",
    "df_sb_all['short_pass_completion_percentage'] = ((df_sb_all['short_passes_completed'] / df_sb_all['short_passes_attempted']) * 100).round(1)\n",
    "df_sb_all['medium_pass_completion_percentage'] = ((df_sb_all['medium_passes_completed'] / df_sb_all['medium_passes_attempted']) * 100).round(1)\n",
    "df_sb_all['long_pass_completion_percentage'] = ((df_sb_all['long_passes_completed'] / df_sb_all['long_passes_attempted']) * 100).round(1)\n",
    "df_sb_all['assist_minus_expected_assist'] = df_sb_all['assists'] - df_sb_all['expected_assists']\n",
    "df_sb_all['tackles_and_interceptions'] = df_sb_all['tackles'] + df_sb_all['interceptions']\n",
    "df_sb_all['tackles_and_dribbles_past'] = (df_sb_all['tackles'] / (df_sb_all['tackles'] + df_sb_all['dribbled_past']) * 100).round(1)\n",
    "df_sb_all['carry_completion_percentage'] = ((df_sb_all['carries_completed'] / df_sb_all['carries']) * 100).round(1)\n",
    "df_sb_all['aerial_duels_win_percentage'] = ((df_sb_all['aerial_duels_won'] / (df_sb_all['aerial_duels_won'] + df_sb_all['aerial_duels_lost'])) * 100).round(1)\n",
    "\n",
    "## Create 'per 90' metrics\n",
    "df_sb_all['tackles_and_interceptions_p90'] = (df_sb_all['tackles_and_interceptions'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['open_play_passes_p90'] = (df_sb_all['open_play_passes'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['carries_p90'] = (df_sb_all['carries'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['carry_length_p90'] = (df_sb_all['carry_length'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['aerial_duels_won_p90'] = (df_sb_all['aerial_duels_won'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['pressured_long_balls_p90'] = (df_sb_all['pressured_long_balls'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['unpressured_long_balls_p90'] = (df_sb_all['unpressured_long_balls'] / df_sb_all['minutes']) * 90\n",
    "df_sb_all['pressures_p90'] = (df_sb_all['pressures'] / df_sb_all['minutes']) * 90\n",
    "\n",
    "## Remove columns\n",
    "df_sb_all = df_sb_all.drop(['min_start', 'min_end'], axis=1)\n",
    "\n",
    "## Round figures\n",
    "df_sb_all['total_distance_progressive_passes'] = df_sb_all['total_distance_progressive_passes'].round(1)\n",
    "df_sb_all['average_distance_progressive_passes'] = df_sb_all['average_distance_progressive_passes'].round(1)\n",
    "df_sb_all['total_distance_completed_passes'] = df_sb_all['total_distance_completed_passes'].round(0)\n",
    "df_sb_all['expected_goals'] = df_sb_all['expected_goals'].round(3)\n",
    "df_sb_all['tackles_and_interceptions_p90'] = df_sb_all['tackles_and_interceptions_p90'].round(2)\n",
    "df_sb_all['open_play_passes_p90'] = df_sb_all['open_play_passes_p90'].round(1)\n",
    "df_sb_all['carries_p90'] = df_sb_all['carries_p90'].round(1)\n",
    "df_sb_all['carry_length_p90'] = df_sb_all['carry_length_p90'].round(1)\n",
    "df_sb_all['aerial_duels_won_p90'] = df_sb_all['aerial_duels_won_p90'].round(1)\n",
    "df_sb_all['pressured_long_balls_p90'] = df_sb_all['pressured_long_balls_p90'].round(1)\n",
    "df_sb_all['unpressured_long_balls_p90'] = df_sb_all['unpressured_long_balls_p90'].round(1)\n",
    "df_sb_all['pressures_p90'] = df_sb_all['pressures_p90'].round(1)\n",
    "\n",
    "## Join player positions to DataFrame\n",
    "df_sb_all = pd.merge(df_sb_all, df_sb_player_positions, how='left', left_on=['player'], right_on=['player'])\n",
    "\n",
    "## Display DataFrame\n",
    "df_sb_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section5.3'>5.3. Export Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_sb_match_by_match.to_csv(data_dir + '/export/' + 'sb_wc2018_events_agg_match_by_match.csv', index=None, header=True)\n",
    "df_sb_all.to_csv(data_dir + '/export/' + 'sb_wc2018_events_agg_all.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section6'></a>\n",
    "\n",
    "## <a id='#section6'>6. Subset Data</a>\n",
    "The following code creates DataFrames for additional Tableau visualisation that are not part of the submission for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.1'>6.1. Extract Lineups from DataFrame</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb['type.name'] column\n",
    "df_sb['type.name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting XI players and formation can be found in the rows where `type.name` is 'Starting XI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup = df_sb[df_sb['type.name'] == 'Starting XI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline DataFrame to include just the columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['id', 'type.name', 'match_date', 'kick_off', 'Full_Fixture_Date', 'team.id', 'team.name', 'tactics.formation', 'tactics.lineup', 'competition_name', 'season_name', 'home_team.home_team_name', 'away_team.away_team_name', 'Team', 'Opponent', 'home_score', 'away_score']\n",
    "\n",
    "## Select only columns of interest\n",
    "df_lineup_select = df_lineup[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the extracted lineup data so far. To get the stating XI players, we need to breakdown the `tactics.lineup` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize tactics.lineup - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_lineup_select_normalize = df_lineup_select.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['tactics.lineup']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    d = pd.json_normalize(df_lineup_select_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_lineup_select_normalize = pd.concat([df_lineup_select_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_lineup_select_normalize.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered = df_lineup_select_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline DataFrame to include just the columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['id', 'match_date', 'kick_off', 'Full_Fixture_Date', 'type.name', 'season_name', 'competition_name', 'home_team.home_team_name', 'away_team.away_team_name', 'Team', 'Opponent', 'home_score', 'away_score', 'tactics.formation', 'tactics.lineup_jersey_number', 'tactics.lineup_position_id', 'tactics.lineup_player_name', 'tactics.lineup_position_name']\n",
    "\n",
    "## Select only columns of interest\n",
    "df_lineup_engineered_select = df_lineup_engineered[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lineup_engineered_select['tactics.formation'] = df_lineup_engineered_select['tactics.formation'].astype('Int64')\n",
    "df_lineup_engineered_select['tactics.lineup_jersey_number'] = df_lineup_engineered_select['tactics.lineup_jersey_number'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.rename(columns={'id': 'Match_Id',\n",
    "                                                                          'match_date': 'Match_Date',\n",
    "                                                                          'kick_off': 'Kick_Off',\n",
    "                                                                          'type.name': 'Type_Name',\n",
    "                                                                          'season_name': 'Season',\n",
    "                                                                          'competition_name': 'Competition',\n",
    "                                                                          'home_team.home_team_name': 'Home_Team',\n",
    "                                                                          'away_team.away_team_name': 'Away_Team',\n",
    "                                                                          'home_score': 'Home_Score',\n",
    "                                                                          'away_score': 'Away_Score',\n",
    "                                                                          'tactics.formation': 'Formation',\n",
    "                                                                          'tactics.lineup_jersey_number': 'Shirt_Number',\n",
    "                                                                          'tactics.lineup_position_id': 'Position_Number',\n",
    "                                                                          'tactics.lineup_player_name': 'Player_Name',\n",
    "                                                                          'tactics.lineup_position_name': 'Position_Name'\n",
    "                                                                         }\n",
    "                                                                         \n",
    "                                                                )\n",
    "\n",
    "## Display DataFrame\n",
    "df_lineup_engineered_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Match_Date from string to datetime64[ns]\n",
    "df_lineup_engineered_select['Match_Date']= pd.to_datetime(df_lineup_engineered_select['Match_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# THIS IS NOT WORKING ATM\n",
    "\n",
    "# Convert Kick_Off from string to datetime64[ns]\n",
    "df_lineup_engineered_select['Kick_Off']= pd.to_datetime(df_lineup_engineered_select['Kick_Off'], format='%H:%M', errors='ignore')\n",
    "df_lineup_engineered_select['Kick_Off'] = df_lineup_engineered_select['Kick_Off'].dt.time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put hyphens between numbers in Formation attribute\n",
    "\n",
    "## Convert Formation attribute from Integer to String\n",
    "df_lineup_engineered_select['Formation'] = df_lineup_engineered_select['Formation'].astype(str)\n",
    "\n",
    "## Define custom function to add hyphen between letters: StackOverflow: https://stackoverflow.com/questions/29382285/python-making-a-function-that-would-add-between-letters\n",
    "def f(s):\n",
    "        m = s[0]\n",
    "        for i in s[1:]:\n",
    "             m += '-' + i\n",
    "        return m\n",
    "    \n",
    "## Apply custom function\n",
    "df_lineup_engineered_select['Formation'] = df_lineup_engineered_select.apply(lambda row: f(row['Formation']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_formation = df_lineup_engineered_select['Formation'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Position Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formations_coords = pd.read_csv(data_dir_sb + '/sb_formation_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_formations_coords['Id'] = df_formations_coords['Id'].astype('Int8')\n",
    "#df_formations_coords['Player_Number'] = df_formations_coords['Player_Number'].astype('Int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select = pd.merge(df_lineup_engineered_select, df_formations_coords, how='left', left_on=['Formation', 'Position_Number'], right_on=['Formation', 'Player_Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lineup_engineered_select = df_lineup_engineered_select.drop(['Player_Number'], axis=1)\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.drop(['Id'], axis=1)\n",
    "df_lineup_engineered_select = df_lineup_engineered_select.drop(['Player_Position'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineup_engineered_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Opponent Data to Each Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['Match_Date',\n",
    "        'Competition',\n",
    "        'Full_Fixture_Date',\n",
    "        'Team',\n",
    "        'Formation'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_lineup_opponent = df_lineup_engineered_select[cols]\n",
    "\n",
    "##\n",
    "df_lineup_opponent = df_lineup_opponent.drop_duplicates()\n",
    "\n",
    "##\n",
    "df_lineup_opponent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join DataFrame to itself on 'Date', 'Fixture', 'Team'/'Opponent', and 'Event', to join Team and Opponent together\n",
    "df_lineup_engineered_opponent_select = pd.merge(df_lineup_engineered_select, df_lineup_opponent,  how='left', left_on=['Match_Date', 'Competition', 'Full_Fixture_Date', 'Opponent'], right_on = ['Match_Date', 'Competition', 'Full_Fixture_Date', 'Team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "\n",
    "## Drop columns\n",
    "df_lineup_engineered_opponent_select = df_lineup_engineered_opponent_select.drop(columns=['Team_y'])\n",
    "\n",
    "\n",
    "## Rename columns\n",
    "df_lineup_engineered_opponent_select = df_lineup_engineered_opponent_select.rename(columns={'Team_x': 'Team',\n",
    "                                                                                            'Formation_x': 'Formation',\n",
    "                                                                                            'Formation_y': 'Opponent_Formation'\n",
    "                                                                                           }\n",
    "                                                                                      )\n",
    "\n",
    "## Display DataFrame\n",
    "df_lineup_engineered_opponent_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_lineup_engineered_opponent_select.to_csv(data_dir_sb + '/lineups/engineered/' + '/sb_lineups_wc2018.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_lineup_engineered_opponent_select.to_csv(data_dir + '/export/' + '/sb_wc2018_lineups.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.2'>6.2. Tactical Shifts</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics = df_sb[df_sb['type.name'] == 'Tactical Shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "##\n",
    "cols = ['id', 'type.name', 'team.id', 'team.name', 'tactics.formation', 'tactics.lineup']\n",
    "\n",
    "##\n",
    "df_tactics_select = df_tactics[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tactics_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize tactics.lineup - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_tactics_select_normalize = df_tactics_select.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['tactics.lineup']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "for col in cols_to_normalize:\n",
    "    \n",
    "    d = pd.json_normalize(df_tactics_select_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_tactics_select_normalize = pd.concat([df_tactics_select_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_tactics_select_normalize.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.3'>6.3. Halves</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half = df_sb[df_sb['type.name'] == 'Half Start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='#section6.4'>6.4. Isolate In-Play Events</a>\n",
    "DataFrame of only player's actions i.e. removing line ups, halves, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section6.4.1'>6.4.1. Remove Non-Event rows</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb['type.name'] column\n",
    "df_sb['type.name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_events = ['Pass', 'Ball Receipt*', 'Carry', 'Duel', 'Miscontrol', 'Pressure', 'Ball Recovery', 'Dribbled Past', 'Dribble', 'Shot', 'Block', 'Goal Keeper', 'Clearance', 'Dispossessed', 'Foul Committed', 'Foul Won', 'Interception', 'Shield', 'Half End', 'Substitution', 'Tactical Shift', 'Injury Stoppage', 'Player Off', 'Player On', 'Offside', 'Referee Ball-Drop', 'Error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events = df_sb[df_sb['type.name'].isin(lst_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section6.4.2'>6.4.2. Break down all `location` attributes into seperate attribute for X, Y (and sometimes Z) coordinates</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all location columns\n",
    "for col in df_sb_events.columns:\n",
    "    if 'location' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the following five 'location' attributes:\n",
    "- `location`\n",
    "- `pass.end_location`\n",
    "- `carry.end_location`\n",
    "- `shot.end_location`\n",
    "- `goalkeeper.end_location`\n",
    "\n",
    "From reviewing the official documentation [[link](https://statsbomb.com/stat-definitions/)], the five attributes have the following dimensionality:\n",
    "- `location` [x, y]\n",
    "- `pass.end_location` [x, y]\n",
    "- `carry.end_location` [x, y]\n",
    "- `shot.end_location` [x, y, z]\n",
    "- `goalkeeper.end_location` [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# CURRENTLY NOT WORKING, NEED TO FIX\n",
    "\n",
    "# Normalize 'shot.freeze_frame' avvtribute - see: https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame\n",
    "\n",
    "## explode all columns with lists of dicts\n",
    "df_sb_events_normalize = df_sb_events.apply(lambda x: x.explode()).reset_index(drop=True)\n",
    "\n",
    "## list of columns with dicts\n",
    "cols_to_normalize = ['shot.freeze_frame']\n",
    "\n",
    "## if there are keys, which will become column names, overlap with excising column names. add the current column name as a prefix\n",
    "normalized = list()\n",
    "\n",
    "for col in cols_to_normalize:\n",
    "    d = pd.json_normalize(df_sb_events_normalize[col], sep='_')\n",
    "    d.columns = [f'{col}_{v}' for v in d.columns]\n",
    "    normalized.append(d.copy())\n",
    "\n",
    "## combine df with the normalized columns\n",
    "df_sb_events_normalize = pd.concat([df_sb_events_normalize] + normalized, axis=1).drop(columns=cols_to_normalize)\n",
    "\n",
    "## display(df_lineup_select_normalize)\n",
    "df_sb_events_normalize.head(30)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_events['location'] = df_sb_events['location'].astype(str)\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].astype(str)\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].astype(str)\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].astype(str)\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].astype(str)\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].astype(str)\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].astype(str)\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "###\n",
    "df_sb_events['location'] = df_sb_events['location'].str.replace('[','')\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].str.replace('[','')\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].str.replace('[','')\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].str.replace('[','')\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].str.replace('[','')\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].str.replace('[','')\n",
    "\n",
    "###\n",
    "df_sb_events['location'] = df_sb_events['location'].str.replace(']','')\n",
    "df_sb_events['pass.end_location'] = df_sb_events['pass.end_location'].str.replace(']','')\n",
    "df_sb_events['carry.end_location'] = df_sb_events['carry.end_location'].str.replace(']','')\n",
    "df_sb_events['shot.end_location'] = df_sb_events['shot.end_location'].str.replace(']','')\n",
    "df_sb_events['goalkeeper.end_location'] = df_sb_events['goalkeeper.end_location'].str.replace(']','')\n",
    "#df_sb_events['shot.freeze_frame'] = df_sb_events['shot.freeze_frame'].str.replace(']','')\n",
    "\n",
    "\n",
    "## Break down each location attributes\n",
    "df_sb_events['location_x'], df_sb_events['location_y'] = df_sb_events['location'].str.split(',', 1).str\n",
    "df_sb_events['pass.end_location_x'], df_sb_events['pass.end_location_y'] = df_sb_events['pass.end_location'].str.split(',', 1).str\n",
    "df_sb_events['carry.end_location_x'], df_sb_events['carry.end_location_y'] = df_sb_events['carry.end_location'].str.split(',', 1).str\n",
    "df_sb_events['shot.end_location_x'], df_sb_events['shot.end_location_y'], df_sb_events['shot.end_location_z'] = df_sb_events['shot.end_location'].str.split(',', 3).str[0:3].str\n",
    "df_sb_events['goalkeeper.end_location_x'], df_sb_events['goalkeeper.end_location_y'] = df_sb_events['goalkeeper.end_location'].str.split(',', 1).str\n",
    "#df_sb_events['shot.freeze_frame_x'], df_sb_events['shot.freeze_frame_y'] = df_sb_events['shot.freeze_frame'].str.split(',', 1).str\n",
    "\n",
    "\n",
    "## Display DataFrame\n",
    "df_sb_events.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_sb_events.to_csv(data_dir_sb + '/engineered/events/' + 'sb_wc2018_events.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_sb_events.to_csv(data_dir + '/export/' + 'sb_wc2018_events.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section6.4.3'>6.4.3. Create Passing Matrix Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_sb_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['df_name'] = 'df1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_sb_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['df_name'] = 'df2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatanate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing['Pass_X'] = np.where(df_sb_events_passing['df_name'] == 'df1', df_sb_events_passing['location_x'], df_sb_events_passing['pass.end_location_x'])\n",
    "df_sb_events_passing['Pass_Y'] = np.where(df_sb_events_passing['df_name'] == 'df1', df_sb_events_passing['location_y'], df_sb_events_passing['pass.end_location_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_events_passing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_sb_events_passing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "#df_sb_events_passing.to_csv(data_dir_sb + '/events/engineered/' + '/sb_wc2018_events_passing_matrix.csv', index=None, header=True)\n",
    "\n",
    "# Export \n",
    "df_sb_events_passing.to_csv(data_dir + '/export/' + '/sb_wc2018_events_passing_matrix.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='#section6.4.4'>6.4.4. Create Passing Network Data</a>\n",
    "\n",
    "See: https://community.tableau.com/s/question/0D54T00000C6YbE/football-passing-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network = df_sb_events_passing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network = df_sb_pass_network[df_sb_pass_network['type.name'] == 'Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network['player_recipient'] = np.where(df_sb_pass_network['df_name'] == 'df1', df_sb_pass_network['player.name'], df_sb_pass_network['pass.recipient.name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_sb_pass_network.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['df_name',\n",
    "        'id',\n",
    "        'index',\n",
    "        'competition_name',\n",
    "        'season_name',\n",
    "        'match_date',\n",
    "        'kick_off',\n",
    "        'Full_Fixture_Date',\n",
    "        'Team',\n",
    "        'Opponent',\n",
    "        'home_team.home_team_name',\n",
    "        'away_team.away_team_name',\n",
    "        'home_score',\n",
    "        'away_score',\n",
    "        'player_recipient',\n",
    "        'player.name',\n",
    "        'pass.recipient.name',\n",
    "        'position.id',\n",
    "        'position.name',\n",
    "        'type.name',\n",
    "        'pass.type.name',\n",
    "        'pass.outcome.name',\n",
    "        'location_x',\n",
    "        'location_y', \n",
    "        'pass.end_location_x',\n",
    "        'pass.end_location_y',\n",
    "        'Pass_X',\n",
    "        'Pass_Y'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_select = df_sb_pass_network[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select['pass.to.from'] = df_sb_pass_network_select['player.name'] + ' - ' + df_sb_pass_network_select['pass.recipient.name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the df_sb_pass_network_select['pass.outcome.name'] column\n",
    "df_sb_pass_network_select['pass.outcome.name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select = df_sb_pass_network_select[df_sb_pass_network_select['pass.outcome.name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select = df_sb_pass_network_select.sort_values(['season_name', 'match_date', 'kick_off', 'Full_Fixture_Date', 'index', 'id', 'df_name'], ascending=[True, True, True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select['Pass_X'] = df_sb_pass_network_select['Pass_X'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['Pass_Y'] = df_sb_pass_network_select['Pass_Y'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['location_x'] = df_sb_pass_network_select['location_x'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['location_y'] = df_sb_pass_network_select['location_y'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['pass.end_location_x'] = df_sb_pass_network_select['pass.end_location_x'].astype(str).astype(float)\n",
    "df_sb_pass_network_select['pass.end_location_y'] = df_sb_pass_network_select['pass.end_location_y'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = (df_sb_pass_network_select\n",
    "                                  .groupby(['competition_name',\n",
    "                                            'season_name',\n",
    "                                            'match_date',\n",
    "                                            'kick_off',\n",
    "                                            'Full_Fixture_Date',\n",
    "                                            'Team',\n",
    "                                            'Opponent',\n",
    "                                            'home_team.home_team_name',\n",
    "                                            'away_team.away_team_name',\n",
    "                                            'home_score',\n",
    "                                            'away_score',\n",
    "                                            'pass.to.from',\n",
    "                                            'player.name',\n",
    "                                            'pass.recipient.name',\n",
    "                                            'player_recipient'\n",
    "                                           ])\n",
    "                                  .agg({'pass.to.from': ['count']\n",
    "                                       })\n",
    "                             )\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped.columns = df_sb_pass_network_grouped.columns.droplevel(level=0)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = df_sb_pass_network_grouped.reset_index()\n",
    "\n",
    "## \n",
    "df_sb_pass_network_grouped.columns = ['competition_name',\n",
    "                                      'season_name',\n",
    "                                      'match_date',\n",
    "                                      'kick_off',\n",
    "                                      'full_fixture_date',\n",
    "                                      'team',\n",
    "                                      'opponent',\n",
    "                                      'home_team_name',\n",
    "                                      'away_team_name',\n",
    "                                      'home_score',\n",
    "                                      'away_score',\n",
    "                                      'pass_to_from',\n",
    "                                      'player_name',\n",
    "                                      'pass_recipient_name',\n",
    "                                      'player_recipient',\n",
    "                                      'count_passes',\n",
    "                                     ]\n",
    "\n",
    "##\n",
    "#df_sb_pass_network_grouped['count_passes'] = df_sb_pass_network_grouped['count_passes'] / 2\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped = df_sb_pass_network_grouped.sort_values(['season_name', 'match_date', 'kick_off', 'full_fixture_date', 'team', 'opponent', 'pass_to_from'], ascending=[True, True, True, True, True, True, True])\n",
    "\n",
    "##\n",
    "df_sb_pass_network_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "\n",
    "## Define columns\n",
    "cols = ['Full_Fixture_Date',\n",
    "        'player.name',\n",
    "        'position.id',\n",
    "        'position.name',\n",
    "        'Pass_X',\n",
    "        'Pass_Y'\n",
    "       ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass = df_sb_pass_network_select[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_avg_pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = (df_sb_pass_network_avg_pass \n",
    "                                          .groupby(['Full_Fixture_Date',\n",
    "                                                    'player.name',\n",
    "                                                    'position.id',\n",
    "                                                    'position.name',\n",
    "                                                   ])\n",
    "                                          .agg({'Pass_X': ['mean'],\n",
    "                                                'Pass_Y': ['mean']\n",
    "                                               })\n",
    "                                     )\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped.columns = df_sb_pass_network_avg_pass_grouped .columns.droplevel(level=0)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = df_sb_pass_network_avg_pass_grouped.reset_index()\n",
    "\n",
    "## \n",
    "df_sb_pass_network_avg_pass_grouped.columns = ['full_fixture_date',\n",
    "                                               'player_name',\n",
    "                                               'position_id',\n",
    "                                               'position_name',\n",
    "                                               'avg_location_pass_x',\n",
    "                                               'avg_location_pass_y'\n",
    "                                     ]\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped['avg_location_pass_x'] = df_sb_pass_network_avg_pass_grouped['avg_location_pass_x'].round(decimals=1)\n",
    "df_sb_pass_network_avg_pass_grouped['avg_location_pass_y'] = df_sb_pass_network_avg_pass_grouped['avg_location_pass_y'].round(decimals=1)\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped = df_sb_pass_network_avg_pass_grouped.sort_values(['full_fixture_date', 'player_name'], ascending=[True, True])\n",
    "\n",
    "##\n",
    "df_sb_pass_network_avg_pass_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Events DataFrame to the Matches DataFrame\n",
    "df_sb_pass_network_final = pd.merge(df_sb_pass_network_grouped, df_sb_pass_network_avg_pass_grouped, left_on=['full_fixture_date', 'player_recipient'], right_on=['full_fixture_date', 'player_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "df_sb_pass_network_final = df_sb_pass_network_final.rename(columns={'player_name_x': 'player_name',\n",
    "                                                                   #'player_name_x': 'player_name'\n",
    "                                                                   }\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sb_pass_network_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb_pass_network_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export \n",
    "df_sb_pass_network_final.to_csv(data_dir_sb + '/engineered/events/' + 'sb_wc2018_events_passing_network.csv', index=None, header=True)\n",
    "\n",
    "# Export \n",
    "df_sb_pass_network_final.to_csv(data_dir + '/export/' + 'sb_wc2018_events_passing_network.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section7'>7. Summary</a>\n",
    "This notebook engineers scraped [StatsBomb](https://statsbomb.com/) data using [pandas](http://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section8'>8. Next Steps</a>\n",
    "The next stage is to visualise this data in Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#section9'>9. References</a>\n",
    "\n",
    "#### Data\n",
    "*    [StatsBomb](https://statsbomb.com/) data\n",
    "*    [StatsBomb](https://github.com/statsbomb/open-data/tree/master/data) open data GitHub repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***Visit my website [eddwebster.com](https://www.eddwebster.com) or my [GitHub Repository](https://github.com/eddwebster) for more projects. If you'd like to get in contact, my Twitter handle is [@eddwebster](http://www.twitter.com/eddwebster) and my email is: edd.j.webster@gmail.com.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 642,
   "position": {
    "height": "40px",
    "left": "1118px",
    "right": "20px",
    "top": "-7px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
